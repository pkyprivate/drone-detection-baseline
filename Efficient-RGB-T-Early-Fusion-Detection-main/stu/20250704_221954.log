2025/07/04 22:19:55 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 882036121
    GPU 0: NVIDIA A100-SXM4-40GB
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 11.8, V11.8.89
    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
    PyTorch: 2.7.1+cu118
    PyTorch compiling details: PyTorch built with:
  - GCC 11.2
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=e2d141dbde55c2a4370fac5165b0561b6af4798b, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/gcc-toolset-11/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.7.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.22.1+cu118
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 882036121
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/07/04 22:19:57 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=16, enable=False)
backend_args = None
data_root = '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV'
dataset_type = 'M3FDDataset'
default_hooks = dict(
    checkpoint=dict(interval=1, type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='DetVisualizationHook'))
default_scope = 'mmdet'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
launcher = 'none'
load_from = 'https://download.openmmlab.com/mmdetection/v2.0/gfl/gfl_r50_fpn_mstrain_2x_coco/gfl_r50_fpn_mstrain_2x_coco_20200629_213802-37bb1edc.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
model = dict(
    backbone=dict(
        depth=50,
        frozen_stages=-1,
        in_channels=3,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=True, type='BN'),
        norm_eval=True,
        num_stages=4,
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        style='pytorch',
        type='ResNetRGBTEarlyModifiedStem'),
    bbox_head=dict(
        anchor_generator=dict(
            octave_base_scale=8,
            ratios=[
                1.0,
            ],
            scales_per_octave=1,
            strides=[
                8,
                16,
                32,
                64,
                128,
            ],
            type='AnchorGenerator'),
        feat_channels=256,
        in_channels=256,
        loss_bbox=dict(loss_weight=2.0, type='GIoULoss'),
        loss_cls=dict(
            beta=2.0,
            loss_weight=1.0,
            type='QualityFocalLoss',
            use_sigmoid=True),
        loss_dfl=dict(loss_weight=0.25, type='DistributionFocalLoss'),
        num_classes=6,
        reg_max=16,
        stacked_convs=4,
        type='GFLHead'),
    corekd_cfg=dict(loss_weight=1, type='MSELoss'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        pad_size_divisor=32,
        rgb_mean=[
            128.2,
            129.3,
            125.3,
        ],
        rgb_std=[
            49.1,
            50.2,
            53.5,
        ],
        thermal_mean=[
            84.1,
            84.1,
            84.1,
        ],
        thermal_std=[
            50.6,
            50.6,
            50.6,
        ],
        type='RGBTDetDataPreprocessor'),
    eval_teacher=True,
    neck=dict(
        add_extra_convs='on_output',
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        num_outs=5,
        out_channels=256,
        start_level=1,
        type='FPN'),
    teacher_ckpt=
    '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/mmdetection/work_dirs/exp1/epoch_12.pth',
    teacher_config=
    '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/mmdetection/configs/gfl/gfl_r50_fpn_1x_m3fd.py',
    test_cfg=dict(
        max_per_img=100,
        min_bbox_size=0,
        nms=dict(iou_threshold=0.6, type='nms'),
        nms_pre=1000,
        score_thr=0.05),
    train_cfg=dict(
        allowed_border=-1,
        assigner=dict(topk=9, type='ATSSAssigner'),
        debug=False,
        pos_weight=-1),
    type='KDGFLCLIP')
optim_wrapper = dict(
    loss_scale='dynamic',
    optimizer=dict(lr=0.01, momentum=0.9, type='SGD', weight_decay=0.0001),
    type='AmpOptimWrapper')
param_scheduler = [
    dict(begin=0, by_epoch=False, end=500, start_factor=0.01, type='LinearLR'),
    dict(
        begin=0,
        by_epoch=True,
        end=12,
        gamma=0.1,
        milestones=[
            8,
            11,
        ],
        type='MultiStepLR'),
]
resume = False
teacher_ckpt = '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/mmdetection/work_dirs/exp1/epoch_12.pth'
teacher_config = '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/mmdetection/configs/gfl/gfl_r50_fpn_1x_m3fd.py'
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=16,
    dataset=dict(
        ann_file=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV/coco/infrared_test.json',
        backend_args=None,
        data_prefix=dict(img=''),
        data_root=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV',
        pipeline=[
            dict(backend_args=None, type='LoadRGBTImageFromFile'),
            dict(keep_ratio=True, scale=(
                640,
                512,
            ), type='ResizeRGBT'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetRGBTInputs'),
        ],
        test_mode=True,
        type='M3FDDataset'),
    drop_last=False,
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    ann_file=
    '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV/coco/infrared_test.json',
    backend_args=None,
    classwise=True,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
test_pipeline = [
    dict(backend_args=None, type='LoadRGBTImageFromFile'),
    dict(keep_ratio=True, scale=(
        640,
        512,
    ), type='ResizeRGBT'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetRGBTInputs'),
]
train_cfg = dict(max_epochs=12, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=16,
    dataset=dict(
        ann_file=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV/coco/infrared_train.json',
        backend_args=None,
        data_prefix=dict(img=''),
        data_root=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV',
        filter_cfg=dict(filter_empty_gt=True, min_size=5),
        pipeline=[
            dict(backend_args=None, type='LoadRGBTImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(keep_ratio=True, scale=(
                640,
                512,
            ), type='ResizeRGBT'),
            dict(prob=0.5, type='RandomFlipRGBT'),
            dict(type='PackDetRGBTInputs'),
        ],
        type='M3FDDataset'),
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadRGBTImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(keep_ratio=True, scale=(
        640,
        512,
    ), type='ResizeRGBT'),
    dict(prob=0.5, type='RandomFlipRGBT'),
    dict(type='PackDetRGBTInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=16,
    dataset=dict(
        ann_file=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV/coco/infrared_test.json',
        backend_args=None,
        data_prefix=dict(img=''),
        data_root=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV',
        pipeline=[
            dict(backend_args=None, type='LoadRGBTImageFromFile'),
            dict(keep_ratio=True, scale=(
                640,
                512,
            ), type='ResizeRGBT'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetRGBTInputs'),
        ],
        test_mode=True,
        type='M3FDDataset'),
    drop_last=False,
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    ann_file=
    '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV/coco/infrared_test.json',
    backend_args=None,
    classwise=True,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/exp2'

2025/07/04 22:20:07 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/07/04 22:20:07 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/07/04 22:20:11 - mmengine - INFO - load model from: torchvision://resnet50
2025/07/04 22:20:11 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
2025/07/04 22:20:11 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

missing keys in source state_dict: stem.conv.weight

Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.stem.conv.weight - torch.Size([3, 6, 7, 7]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

neck.fpn_convs.4.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.4.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.cls_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.cls_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.cls_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.cls_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.cls_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.cls_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.3.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.cls_convs.3.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.reg_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.reg_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.reg_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.reg_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.reg_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.reg_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.3.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.reg_convs.3.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.gfl_cls.weight - torch.Size([6, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

bbox_head.gfl_cls.bias - torch.Size([6]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

bbox_head.gfl_reg.weight - torch.Size([68, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.gfl_reg.bias - torch.Size([68]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.scales.0.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.scales.1.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.scales.2.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.scales.3.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

bbox_head.scales.4.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

match_convs.0.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

match_convs.1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

match_convs.2.weight - torch.Size([256, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

resnet_fc.weight - torch.Size([6, 256]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

resnet_mask_conv.0.weight - torch.Size([6, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

resnet_mask_conv.1.weight - torch.Size([6]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

resnet_mask_conv.1.bias - torch.Size([6]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

resnet_mask_conv.3.weight - torch.Size([6, 6, 3, 3]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

resnet_mask_conv.4.weight - torch.Size([6]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

resnet_mask_conv.4.bias - torch.Size([6]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.positional_embedding - torch.Size([77, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.text_projection - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.logit_scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.class_embedding - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.positional_embedding - torch.Size([50, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.proj - torch.Size([768, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.conv1.weight - torch.Size([768, 3, 32, 32]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.ln_pre.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.ln_pre.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.0.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.1.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.2.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.3.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.4.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.5.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.6.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.7.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.8.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.9.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.10.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.transformer.resblocks.11.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.ln_post.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.visual.ln_post.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.0.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.1.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.2.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.3.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.4.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.5.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.6.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.7.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.8.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.9.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.10.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.transformer.resblocks.11.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.token_embedding.weight - torch.Size([49408, 512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.ln_final.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_model.ln_final.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_fc.0.weight - torch.Size([3, 6]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_fc.1.weight - torch.Size([3]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_fc.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_fc.3.weight - torch.Size([6, 3]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_fc.4.weight - torch.Size([6]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_fc.4.bias - torch.Size([6]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  

clip_fc.7.weight - torch.Size([6, 6]): 
The value is the same before and after calling `init_weights` of KDGFLCLIP  
2025/07/04 22:20:11 - mmengine - INFO - Load checkpoint from https://download.openmmlab.com/mmdetection/v2.0/gfl/gfl_r50_fpn_mstrain_2x_coco/gfl_r50_fpn_mstrain_2x_coco_20200629_213802-37bb1edc.pth
2025/07/04 22:20:11 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/07/04 22:20:11 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/07/04 22:20:11 - mmengine - INFO - Checkpoints will be saved to /root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/mmdetection/work_dirs/exp2.
2025/07/04 22:20:54 - mmengine - INFO - Epoch(train)  [1][ 50/491]  lr: 1.0721e-03  eta: 1:23:14  time: 0.8549  data_time: 0.0490  memory: 18143  loss: 8.7087  loss_cls: 2.7201  loss_bbox: 0.8697  loss_dfl: 0.2262  backbone_loss_mask: 0.8998  clip_transfer_loss: 0.6470  kd_losses: 3.3458
2025/07/04 22:21:34 - mmengine - INFO - Epoch(train)  [1][100/491]  lr: 2.0641e-03  eta: 1:20:16  time: 0.8082  data_time: 0.0426  memory: 18143  loss: 3.9749  loss_cls: 1.1557  loss_bbox: 0.6551  loss_dfl: 0.1887  backbone_loss_mask: 0.8925  clip_transfer_loss: 0.5955  kd_losses: 0.4874
2025/07/04 22:22:15 - mmengine - INFO - Epoch(train)  [1][150/491]  lr: 3.0561e-03  eta: 1:18:48  time: 0.8076  data_time: 0.0415  memory: 18143  loss: 3.6594  loss_cls: 1.0062  loss_bbox: 0.6645  loss_dfl: 0.1803  backbone_loss_mask: 0.8793  clip_transfer_loss: 0.4926  kd_losses: 0.4364
2025/07/04 22:22:56 - mmengine - INFO - Epoch(train)  [1][200/491]  lr: 4.0481e-03  eta: 1:18:08  time: 0.8240  data_time: 0.0427  memory: 18143  loss: 3.3178  loss_cls: 0.8979  loss_bbox: 0.6595  loss_dfl: 0.1824  backbone_loss_mask: 0.8607  clip_transfer_loss: 0.3462  kd_losses: 0.3712
2025/07/04 22:23:36 - mmengine - INFO - Epoch(train)  [1][250/491]  lr: 5.0401e-03  eta: 1:17:04  time: 0.8034  data_time: 0.0407  memory: 18143  loss: 3.1208  loss_cls: 0.7763  loss_bbox: 0.7074  loss_dfl: 0.1872  backbone_loss_mask: 0.8373  clip_transfer_loss: 0.2850  kd_losses: 0.3275
2025/07/04 22:24:21 - mmengine - INFO - Epoch(train)  [1][300/491]  lr: 6.0321e-03  eta: 1:17:26  time: 0.8878  data_time: 0.0413  memory: 18143  loss: 3.0845  loss_cls: 0.8097  loss_bbox: 0.7020  loss_dfl: 0.1881  backbone_loss_mask: 0.8124  clip_transfer_loss: 0.2430  kd_losses: 0.3292
2025/07/04 22:25:00 - mmengine - INFO - Epoch(train)  [1][350/491]  lr: 7.0240e-03  eta: 1:16:16  time: 0.7944  data_time: 0.0411  memory: 18143  loss: 2.7198  loss_cls: 0.6018  loss_bbox: 0.6683  loss_dfl: 0.1836  backbone_loss_mask: 0.7850  clip_transfer_loss: 0.2130  kd_losses: 0.2682
2025/07/04 22:25:40 - mmengine - INFO - Epoch(train)  [1][400/491]  lr: 8.0160e-03  eta: 1:15:13  time: 0.7937  data_time: 0.0404  memory: 18143  loss: 2.5419  loss_cls: 0.5523  loss_bbox: 0.6418  loss_dfl: 0.1796  backbone_loss_mask: 0.7575  clip_transfer_loss: 0.1787  kd_losses: 0.2319
2025/07/04 22:26:20 - mmengine - INFO - Epoch(train)  [1][450/491]  lr: 9.0080e-03  eta: 1:14:18  time: 0.7999  data_time: 0.0362  memory: 18143  loss: 2.6556  loss_cls: 0.6211  loss_bbox: 0.7082  loss_dfl: 0.1912  backbone_loss_mask: 0.7312  clip_transfer_loss: 0.1658  kd_losses: 0.2380
2025/07/04 22:26:51 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 22:26:51 - mmengine - INFO - Saving checkpoint at 1 epochs
2025/07/04 22:27:04 - mmengine - INFO - Epoch(val)  [1][ 50/211]    eta: 0:00:33  time: 0.2079  data_time: 0.0515  memory: 18143  
2025/07/04 22:27:14 - mmengine - INFO - Epoch(val)  [1][100/211]    eta: 0:00:22  time: 0.1940  data_time: 0.0392  memory: 2253  
2025/07/04 22:27:24 - mmengine - INFO - Epoch(val)  [1][150/211]    eta: 0:00:12  time: 0.1945  data_time: 0.0400  memory: 2253  
2025/07/04 22:27:33 - mmengine - INFO - Epoch(val)  [1][200/211]    eta: 0:00:02  time: 0.1920  data_time: 0.0376  memory: 2253  
2025/07/04 22:27:38 - mmengine - INFO - Evaluating bbox...
2025/07/04 22:27:49 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.172 | 0.506  | 0.061  | 0.168 | 0.375 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 22:27:49 - mmengine - INFO - bbox_mAP_copypaste: 0.172 0.506 0.061 0.168 0.375 -1.000
2025/07/04 22:27:49 - mmengine - INFO - Epoch(val) [1][211/211]    coco/UAV_precision: 0.1720  coco/bbox_mAP: 0.1720  coco/bbox_mAP_50: 0.5060  coco/bbox_mAP_75: 0.0610  coco/bbox_mAP_s: 0.1680  coco/bbox_mAP_m: 0.3750  coco/bbox_mAP_l: -1.0000  data_time: 0.0417  time: 0.1964
2025/07/04 22:28:28 - mmengine - INFO - Epoch(train)  [2][ 50/491]  lr: 1.0000e-02  eta: 1:12:22  time: 0.7754  data_time: 0.0417  memory: 18143  loss: 2.5494  loss_cls: 0.6136  loss_bbox: 0.6979  loss_dfl: 0.1846  backbone_loss_mask: 0.6879  clip_transfer_loss: 0.1334  kd_losses: 0.2319
2025/07/04 22:29:07 - mmengine - INFO - Epoch(train)  [2][100/491]  lr: 1.0000e-02  eta: 1:11:20  time: 0.7654  data_time: 0.0356  memory: 18143  loss: 2.4671  loss_cls: 0.5820  loss_bbox: 0.6763  loss_dfl: 0.1796  backbone_loss_mask: 0.6687  clip_transfer_loss: 0.1223  kd_losses: 0.2382
2025/07/04 22:29:45 - mmengine - INFO - Epoch(train)  [2][150/491]  lr: 1.0000e-02  eta: 1:10:27  time: 0.7746  data_time: 0.0354  memory: 18143  loss: 2.3903  loss_cls: 0.5405  loss_bbox: 0.6734  loss_dfl: 0.1819  backbone_loss_mask: 0.6530  clip_transfer_loss: 0.1302  kd_losses: 0.2114
2025/07/04 22:30:23 - mmengine - INFO - Epoch(train)  [2][200/491]  lr: 1.0000e-02  eta: 1:09:28  time: 0.7558  data_time: 0.0314  memory: 18143  loss: 2.3446  loss_cls: 0.5392  loss_bbox: 0.6769  loss_dfl: 0.1834  backbone_loss_mask: 0.6387  clip_transfer_loss: 0.1114  kd_losses: 0.1950
2025/07/04 22:31:01 - mmengine - INFO - Epoch(train)  [2][250/491]  lr: 1.0000e-02  eta: 1:08:36  time: 0.7669  data_time: 0.0381  memory: 18143  loss: 2.2069  loss_cls: 0.4932  loss_bbox: 0.6235  loss_dfl: 0.1798  backbone_loss_mask: 0.6265  clip_transfer_loss: 0.1135  kd_losses: 0.1704
2025/07/04 22:31:39 - mmengine - INFO - Epoch(train)  [2][300/491]  lr: 1.0000e-02  eta: 1:07:42  time: 0.7571  data_time: 0.0393  memory: 18143  loss: 2.1606  loss_cls: 0.4695  loss_bbox: 0.6402  loss_dfl: 0.1769  backbone_loss_mask: 0.6159  clip_transfer_loss: 0.1107  kd_losses: 0.1474
2025/07/04 22:32:19 - mmengine - INFO - Epoch(train)  [2][350/491]  lr: 1.0000e-02  eta: 1:07:01  time: 0.7902  data_time: 0.0411  memory: 18143  loss: 2.1647  loss_cls: 0.4615  loss_bbox: 0.6495  loss_dfl: 0.1794  backbone_loss_mask: 0.6071  clip_transfer_loss: 0.1211  kd_losses: 0.1462
2025/07/04 22:32:57 - mmengine - INFO - Epoch(train)  [2][400/491]  lr: 1.0000e-02  eta: 1:06:12  time: 0.7661  data_time: 0.0411  memory: 18143  loss: 2.0399  loss_cls: 0.3839  loss_bbox: 0.6281  loss_dfl: 0.1804  backbone_loss_mask: 0.5990  clip_transfer_loss: 0.1054  kd_losses: 0.1431
2025/07/04 22:33:35 - mmengine - INFO - Epoch(train)  [2][450/491]  lr: 1.0000e-02  eta: 1:05:26  time: 0.7679  data_time: 0.0406  memory: 18143  loss: 2.0089  loss_cls: 0.3904  loss_bbox: 0.6035  loss_dfl: 0.1757  backbone_loss_mask: 0.5918  clip_transfer_loss: 0.1164  kd_losses: 0.1312
2025/07/04 22:34:06 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 22:34:06 - mmengine - INFO - Saving checkpoint at 2 epochs
2025/07/04 22:34:18 - mmengine - INFO - Epoch(val)  [2][ 50/211]    eta: 0:00:31  time: 0.1937  data_time: 0.0383  memory: 18143  
2025/07/04 22:34:27 - mmengine - INFO - Epoch(val)  [2][100/211]    eta: 0:00:21  time: 0.1870  data_time: 0.0341  memory: 2253  
2025/07/04 22:34:37 - mmengine - INFO - Epoch(val)  [2][150/211]    eta: 0:00:11  time: 0.1890  data_time: 0.0355  memory: 2253  
2025/07/04 22:34:46 - mmengine - INFO - Epoch(val)  [2][200/211]    eta: 0:00:02  time: 0.1858  data_time: 0.0329  memory: 2253  
2025/07/04 22:34:50 - mmengine - INFO - Evaluating bbox...
2025/07/04 22:34:57 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.253 | 0.692  | 0.115  | 0.248 | 0.506 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 22:34:57 - mmengine - INFO - bbox_mAP_copypaste: 0.253 0.692 0.115 0.248 0.506 -1.000
2025/07/04 22:34:57 - mmengine - INFO - Epoch(val) [2][211/211]    coco/UAV_precision: 0.2530  coco/bbox_mAP: 0.2530  coco/bbox_mAP_50: 0.6920  coco/bbox_mAP_75: 0.1150  coco/bbox_mAP_s: 0.2480  coco/bbox_mAP_m: 0.5060  coco/bbox_mAP_l: -1.0000  data_time: 0.0348  time: 0.1874
2025/07/04 22:35:11 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 22:35:35 - mmengine - INFO - Epoch(train)  [3][ 50/491]  lr: 1.0000e-02  eta: 1:03:55  time: 0.7577  data_time: 0.0421  memory: 18143  loss: 1.9408  loss_cls: 0.3619  loss_bbox: 0.5880  loss_dfl: 0.1707  backbone_loss_mask: 0.5806  clip_transfer_loss: 0.1158  kd_losses: 0.1240
2025/07/04 22:36:13 - mmengine - INFO - Epoch(train)  [3][100/491]  lr: 1.0000e-02  eta: 1:03:08  time: 0.7554  data_time: 0.0373  memory: 18143  loss: 1.9700  loss_cls: 0.3769  loss_bbox: 0.6065  loss_dfl: 0.1743  backbone_loss_mask: 0.5751  clip_transfer_loss: 0.1060  kd_losses: 0.1312
2025/07/04 22:36:50 - mmengine - INFO - Epoch(train)  [3][150/491]  lr: 1.0000e-02  eta: 1:02:20  time: 0.7440  data_time: 0.0334  memory: 18143  loss: 1.9573  loss_cls: 0.3764  loss_bbox: 0.6098  loss_dfl: 0.1773  backbone_loss_mask: 0.5698  clip_transfer_loss: 0.1060  kd_losses: 0.1179
2025/07/04 22:37:27 - mmengine - INFO - Epoch(train)  [3][200/491]  lr: 1.0000e-02  eta: 1:01:33  time: 0.7466  data_time: 0.0330  memory: 18143  loss: 1.9574  loss_cls: 0.3669  loss_bbox: 0.6320  loss_dfl: 0.1770  backbone_loss_mask: 0.5657  clip_transfer_loss: 0.1000  kd_losses: 0.1160
2025/07/04 22:38:04 - mmengine - INFO - Epoch(train)  [3][250/491]  lr: 1.0000e-02  eta: 1:00:45  time: 0.7416  data_time: 0.0313  memory: 18143  loss: 1.9653  loss_cls: 0.3959  loss_bbox: 0.6052  loss_dfl: 0.1692  backbone_loss_mask: 0.5615  clip_transfer_loss: 0.1174  kd_losses: 0.1161
2025/07/04 22:38:42 - mmengine - INFO - Epoch(train)  [3][300/491]  lr: 1.0000e-02  eta: 1:00:00  time: 0.7480  data_time: 0.0369  memory: 18143  loss: 1.8322  loss_cls: 0.3189  loss_bbox: 0.5654  loss_dfl: 0.1698  backbone_loss_mask: 0.5567  clip_transfer_loss: 0.1118  kd_losses: 0.1095
2025/07/04 22:39:19 - mmengine - INFO - Epoch(train)  [3][350/491]  lr: 1.0000e-02  eta: 0:59:16  time: 0.7541  data_time: 0.0384  memory: 18143  loss: 1.9263  loss_cls: 0.3750  loss_bbox: 0.5977  loss_dfl: 0.1687  backbone_loss_mask: 0.5537  clip_transfer_loss: 0.1063  kd_losses: 0.1249
2025/07/04 22:39:57 - mmengine - INFO - Epoch(train)  [3][400/491]  lr: 1.0000e-02  eta: 0:58:33  time: 0.7518  data_time: 0.0400  memory: 18143  loss: 1.8291  loss_cls: 0.3311  loss_bbox: 0.5768  loss_dfl: 0.1633  backbone_loss_mask: 0.5503  clip_transfer_loss: 0.0989  kd_losses: 0.1087
2025/07/04 22:40:34 - mmengine - INFO - Epoch(train)  [3][450/491]  lr: 1.0000e-02  eta: 0:57:48  time: 0.7439  data_time: 0.0308  memory: 18143  loss: 1.8542  loss_cls: 0.3588  loss_bbox: 0.5666  loss_dfl: 0.1679  backbone_loss_mask: 0.5463  clip_transfer_loss: 0.1015  kd_losses: 0.1131
2025/07/04 22:41:04 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 22:41:04 - mmengine - INFO - Saving checkpoint at 3 epochs
2025/07/04 22:41:16 - mmengine - INFO - Epoch(val)  [3][ 50/211]    eta: 0:00:31  time: 0.1929  data_time: 0.0383  memory: 18143  
2025/07/04 22:41:26 - mmengine - INFO - Epoch(val)  [3][100/211]    eta: 0:00:21  time: 0.1878  data_time: 0.0346  memory: 2253  
2025/07/04 22:41:35 - mmengine - INFO - Epoch(val)  [3][150/211]    eta: 0:00:11  time: 0.1892  data_time: 0.0353  memory: 2253  
2025/07/04 22:41:44 - mmengine - INFO - Epoch(val)  [3][200/211]    eta: 0:00:02  time: 0.1881  data_time: 0.0343  memory: 2253  
2025/07/04 22:41:49 - mmengine - INFO - Evaluating bbox...
2025/07/04 22:41:57 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.325 | 0.797  | 0.183  | 0.322 | 0.468 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 22:41:57 - mmengine - INFO - bbox_mAP_copypaste: 0.325 0.797 0.183 0.322 0.468 -1.000
2025/07/04 22:41:57 - mmengine - INFO - Epoch(val) [3][211/211]    coco/UAV_precision: 0.3250  coco/bbox_mAP: 0.3250  coco/bbox_mAP_50: 0.7970  coco/bbox_mAP_75: 0.1830  coco/bbox_mAP_s: 0.3220  coco/bbox_mAP_m: 0.4680  coco/bbox_mAP_l: -1.0000  data_time: 0.0353  time: 0.1879
2025/07/04 22:42:35 - mmengine - INFO - Epoch(train)  [4][ 50/491]  lr: 1.0000e-02  eta: 0:56:28  time: 0.7516  data_time: 0.0377  memory: 18143  loss: 1.7386  loss_cls: 0.2879  loss_bbox: 0.5450  loss_dfl: 0.1655  backbone_loss_mask: 0.5404  clip_transfer_loss: 0.0964  kd_losses: 0.1034
2025/07/04 22:43:13 - mmengine - INFO - Epoch(train)  [4][100/491]  lr: 1.0000e-02  eta: 0:55:46  time: 0.7564  data_time: 0.0423  memory: 18143  loss: 1.8178  loss_cls: 0.3227  loss_bbox: 0.5829  loss_dfl: 0.1701  backbone_loss_mask: 0.5374  clip_transfer_loss: 0.0947  kd_losses: 0.1098
2025/07/04 22:43:50 - mmengine - INFO - Epoch(train)  [4][150/491]  lr: 1.0000e-02  eta: 0:55:04  time: 0.7469  data_time: 0.0395  memory: 18143  loss: 1.8223  loss_cls: 0.3344  loss_bbox: 0.5790  loss_dfl: 0.1660  backbone_loss_mask: 0.5345  clip_transfer_loss: 0.1010  kd_losses: 0.1073
2025/07/04 22:44:28 - mmengine - INFO - Epoch(train)  [4][200/491]  lr: 1.0000e-02  eta: 0:54:23  time: 0.7580  data_time: 0.0404  memory: 18143  loss: 1.7699  loss_cls: 0.3196  loss_bbox: 0.5579  loss_dfl: 0.1660  backbone_loss_mask: 0.5310  clip_transfer_loss: 0.0965  kd_losses: 0.0989
2025/07/04 22:45:05 - mmengine - INFO - Epoch(train)  [4][250/491]  lr: 1.0000e-02  eta: 0:53:41  time: 0.7473  data_time: 0.0348  memory: 18143  loss: 1.7807  loss_cls: 0.3333  loss_bbox: 0.5567  loss_dfl: 0.1637  backbone_loss_mask: 0.5288  clip_transfer_loss: 0.0966  kd_losses: 0.1015
2025/07/04 22:45:43 - mmengine - INFO - Epoch(train)  [4][300/491]  lr: 1.0000e-02  eta: 0:52:59  time: 0.7408  data_time: 0.0356  memory: 18143  loss: 1.8113  loss_cls: 0.3401  loss_bbox: 0.5585  loss_dfl: 0.1667  backbone_loss_mask: 0.5257  clip_transfer_loss: 0.1053  kd_losses: 0.1151
2025/07/04 22:46:20 - mmengine - INFO - Epoch(train)  [4][350/491]  lr: 1.0000e-02  eta: 0:52:19  time: 0.7582  data_time: 0.0395  memory: 18143  loss: 1.8271  loss_cls: 0.3578  loss_bbox: 0.5722  loss_dfl: 0.1680  backbone_loss_mask: 0.5243  clip_transfer_loss: 0.0939  kd_losses: 0.1108
2025/07/04 22:46:58 - mmengine - INFO - Epoch(train)  [4][400/491]  lr: 1.0000e-02  eta: 0:51:38  time: 0.7536  data_time: 0.0362  memory: 18143  loss: 1.7592  loss_cls: 0.3159  loss_bbox: 0.5540  loss_dfl: 0.1622  backbone_loss_mask: 0.5194  clip_transfer_loss: 0.1026  kd_losses: 0.1051
2025/07/04 22:47:35 - mmengine - INFO - Epoch(train)  [4][450/491]  lr: 1.0000e-02  eta: 0:50:57  time: 0.7467  data_time: 0.0329  memory: 18143  loss: 1.7250  loss_cls: 0.3064  loss_bbox: 0.5501  loss_dfl: 0.1656  backbone_loss_mask: 0.5165  clip_transfer_loss: 0.0912  kd_losses: 0.0951
2025/07/04 22:48:06 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 22:48:06 - mmengine - INFO - Saving checkpoint at 4 epochs
2025/07/04 22:48:18 - mmengine - INFO - Epoch(val)  [4][ 50/211]    eta: 0:00:31  time: 0.1950  data_time: 0.0407  memory: 18143  
2025/07/04 22:48:28 - mmengine - INFO - Epoch(val)  [4][100/211]    eta: 0:00:21  time: 0.1911  data_time: 0.0372  memory: 2253  
2025/07/04 22:48:37 - mmengine - INFO - Epoch(val)  [4][150/211]    eta: 0:00:11  time: 0.1900  data_time: 0.0370  memory: 2253  
2025/07/04 22:48:47 - mmengine - INFO - Epoch(val)  [4][200/211]    eta: 0:00:02  time: 0.1920  data_time: 0.0383  memory: 2253  
2025/07/04 22:48:51 - mmengine - INFO - Evaluating bbox...
2025/07/04 22:49:00 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.306 | 0.778  | 0.16   | 0.299 | 0.57  | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 22:49:00 - mmengine - INFO - bbox_mAP_copypaste: 0.306 0.778 0.160 0.299 0.570 -1.000
2025/07/04 22:49:01 - mmengine - INFO - Epoch(val) [4][211/211]    coco/UAV_precision: 0.3060  coco/bbox_mAP: 0.3060  coco/bbox_mAP_50: 0.7780  coco/bbox_mAP_75: 0.1600  coco/bbox_mAP_s: 0.2990  coco/bbox_mAP_m: 0.5700  coco/bbox_mAP_l: -1.0000  data_time: 0.0378  time: 0.1902
2025/07/04 22:49:28 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 22:49:39 - mmengine - INFO - Epoch(train)  [5][ 50/491]  lr: 1.0000e-02  eta: 0:49:44  time: 0.7632  data_time: 0.0387  memory: 18143  loss: 1.7077  loss_cls: 0.2883  loss_bbox: 0.5587  loss_dfl: 0.1638  backbone_loss_mask: 0.5106  clip_transfer_loss: 0.0972  kd_losses: 0.0891
2025/07/04 22:50:16 - mmengine - INFO - Epoch(train)  [5][100/491]  lr: 1.0000e-02  eta: 0:49:03  time: 0.7440  data_time: 0.0348  memory: 18143  loss: 1.6385  loss_cls: 0.2650  loss_bbox: 0.5243  loss_dfl: 0.1636  backbone_loss_mask: 0.5058  clip_transfer_loss: 0.0944  kd_losses: 0.0853
2025/07/04 22:50:53 - mmengine - INFO - Epoch(train)  [5][150/491]  lr: 1.0000e-02  eta: 0:48:23  time: 0.7486  data_time: 0.0342  memory: 18143  loss: 1.6514  loss_cls: 0.2673  loss_bbox: 0.5389  loss_dfl: 0.1621  backbone_loss_mask: 0.5064  clip_transfer_loss: 0.0872  kd_losses: 0.0895
2025/07/04 22:51:31 - mmengine - INFO - Epoch(train)  [5][200/491]  lr: 1.0000e-02  eta: 0:47:43  time: 0.7515  data_time: 0.0325  memory: 18143  loss: 1.5572  loss_cls: 0.2207  loss_bbox: 0.5094  loss_dfl: 0.1611  backbone_loss_mask: 0.4982  clip_transfer_loss: 0.0937  kd_losses: 0.0742
2025/07/04 22:52:08 - mmengine - INFO - Epoch(train)  [5][250/491]  lr: 1.0000e-02  eta: 0:47:03  time: 0.7505  data_time: 0.0376  memory: 18143  loss: 1.6612  loss_cls: 0.2790  loss_bbox: 0.5541  loss_dfl: 0.1622  backbone_loss_mask: 0.4959  clip_transfer_loss: 0.0936  kd_losses: 0.0764
2025/07/04 22:52:46 - mmengine - INFO - Epoch(train)  [5][300/491]  lr: 1.0000e-02  eta: 0:46:23  time: 0.7408  data_time: 0.0346  memory: 18143  loss: 1.6593  loss_cls: 0.2814  loss_bbox: 0.5395  loss_dfl: 0.1625  backbone_loss_mask: 0.4931  clip_transfer_loss: 0.0713  kd_losses: 0.1116
2025/07/04 22:53:24 - mmengine - INFO - Epoch(train)  [5][350/491]  lr: 1.0000e-02  eta: 0:45:44  time: 0.7637  data_time: 0.0399  memory: 18143  loss: 1.6117  loss_cls: 0.2570  loss_bbox: 0.5398  loss_dfl: 0.1662  backbone_loss_mask: 0.4906  clip_transfer_loss: 0.0775  kd_losses: 0.0806
2025/07/04 22:54:01 - mmengine - INFO - Epoch(train)  [5][400/491]  lr: 1.0000e-02  eta: 0:45:04  time: 0.7396  data_time: 0.0339  memory: 18143  loss: 1.7008  loss_cls: 0.3117  loss_bbox: 0.5612  loss_dfl: 0.1662  backbone_loss_mask: 0.4885  clip_transfer_loss: 0.0820  kd_losses: 0.0913
2025/07/04 22:54:39 - mmengine - INFO - Epoch(train)  [5][450/491]  lr: 1.0000e-02  eta: 0:44:25  time: 0.7595  data_time: 0.0381  memory: 18143  loss: 1.6600  loss_cls: 0.2983  loss_bbox: 0.5445  loss_dfl: 0.1630  backbone_loss_mask: 0.4808  clip_transfer_loss: 0.0807  kd_losses: 0.0927
2025/07/04 22:55:09 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 22:55:09 - mmengine - INFO - Saving checkpoint at 5 epochs
2025/07/04 22:55:21 - mmengine - INFO - Epoch(val)  [5][ 50/211]    eta: 0:00:31  time: 0.1952  data_time: 0.0410  memory: 18143  
2025/07/04 22:55:31 - mmengine - INFO - Epoch(val)  [5][100/211]    eta: 0:00:21  time: 0.1923  data_time: 0.0383  memory: 2253  
2025/07/04 22:55:41 - mmengine - INFO - Epoch(val)  [5][150/211]    eta: 0:00:11  time: 0.1944  data_time: 0.0395  memory: 2253  
2025/07/04 22:55:50 - mmengine - INFO - Epoch(val)  [5][200/211]    eta: 0:00:02  time: 0.1986  data_time: 0.0429  memory: 2253  
2025/07/04 22:55:53 - mmengine - INFO - Evaluating bbox...
2025/07/04 22:55:56 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.316 | 0.756  | 0.203  | 0.31  | 0.616 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 22:55:56 - mmengine - INFO - bbox_mAP_copypaste: 0.316 0.756 0.203 0.310 0.616 -1.000
2025/07/04 22:55:56 - mmengine - INFO - Epoch(val) [5][211/211]    coco/UAV_precision: 0.3160  coco/bbox_mAP: 0.3160  coco/bbox_mAP_50: 0.7560  coco/bbox_mAP_75: 0.2030  coco/bbox_mAP_s: 0.3100  coco/bbox_mAP_m: 0.6160  coco/bbox_mAP_l: -1.0000  data_time: 0.0399  time: 0.1934
2025/07/04 22:56:35 - mmengine - INFO - Epoch(train)  [6][ 50/491]  lr: 1.0000e-02  eta: 0:43:14  time: 0.7764  data_time: 0.0407  memory: 18143  loss: 1.7854  loss_cls: 0.4142  loss_bbox: 0.5460  loss_dfl: 0.1622  backbone_loss_mask: 0.4755  clip_transfer_loss: 0.0756  kd_losses: 0.1119
2025/07/04 22:57:13 - mmengine - INFO - Epoch(train)  [6][100/491]  lr: 1.0000e-02  eta: 0:42:36  time: 0.7672  data_time: 0.0366  memory: 18143  loss: 1.6113  loss_cls: 0.2822  loss_bbox: 0.5256  loss_dfl: 0.1651  backbone_loss_mask: 0.4681  clip_transfer_loss: 0.0748  kd_losses: 0.0955
2025/07/04 22:57:51 - mmengine - INFO - Epoch(train)  [6][150/491]  lr: 1.0000e-02  eta: 0:41:57  time: 0.7586  data_time: 0.0373  memory: 18143  loss: 1.6047  loss_cls: 0.2729  loss_bbox: 0.5420  loss_dfl: 0.1623  backbone_loss_mask: 0.4628  clip_transfer_loss: 0.0762  kd_losses: 0.0885
2025/07/04 22:58:29 - mmengine - INFO - Epoch(train)  [6][200/491]  lr: 1.0000e-02  eta: 0:41:18  time: 0.7543  data_time: 0.0385  memory: 18143  loss: 1.5750  loss_cls: 0.2546  loss_bbox: 0.5362  loss_dfl: 0.1641  backbone_loss_mask: 0.4589  clip_transfer_loss: 0.0829  kd_losses: 0.0782
2025/07/04 22:59:07 - mmengine - INFO - Epoch(train)  [6][250/491]  lr: 1.0000e-02  eta: 0:40:40  time: 0.7611  data_time: 0.0379  memory: 18143  loss: 1.5718  loss_cls: 0.2668  loss_bbox: 0.5258  loss_dfl: 0.1620  backbone_loss_mask: 0.4540  clip_transfer_loss: 0.0808  kd_losses: 0.0825
2025/07/04 22:59:46 - mmengine - INFO - Epoch(train)  [6][300/491]  lr: 1.0000e-02  eta: 0:40:02  time: 0.7734  data_time: 0.0402  memory: 18143  loss: 1.5333  loss_cls: 0.2322  loss_bbox: 0.5248  loss_dfl: 0.1610  backbone_loss_mask: 0.4473  clip_transfer_loss: 0.0857  kd_losses: 0.0824
2025/07/04 23:00:24 - mmengine - INFO - Epoch(train)  [6][350/491]  lr: 1.0000e-02  eta: 0:39:24  time: 0.7641  data_time: 0.0379  memory: 18143  loss: 1.5049  loss_cls: 0.2390  loss_bbox: 0.5167  loss_dfl: 0.1582  backbone_loss_mask: 0.4394  clip_transfer_loss: 0.0786  kd_losses: 0.0731
2025/07/04 23:01:02 - mmengine - INFO - Epoch(train)  [6][400/491]  lr: 1.0000e-02  eta: 0:38:45  time: 0.7661  data_time: 0.0388  memory: 18143  loss: 1.4914  loss_cls: 0.2223  loss_bbox: 0.5292  loss_dfl: 0.1612  backbone_loss_mask: 0.4273  clip_transfer_loss: 0.0784  kd_losses: 0.0730
2025/07/04 23:01:40 - mmengine - INFO - Epoch(train)  [6][450/491]  lr: 1.0000e-02  eta: 0:38:07  time: 0.7631  data_time: 0.0411  memory: 18143  loss: 1.4776  loss_cls: 0.2410  loss_bbox: 0.5069  loss_dfl: 0.1583  backbone_loss_mask: 0.4242  clip_transfer_loss: 0.0753  kd_losses: 0.0720
2025/07/04 23:02:11 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 23:02:11 - mmengine - INFO - Saving checkpoint at 6 epochs
2025/07/04 23:02:24 - mmengine - INFO - Epoch(val)  [6][ 50/211]    eta: 0:00:31  time: 0.1982  data_time: 0.0419  memory: 18143  
2025/07/04 23:02:33 - mmengine - INFO - Epoch(val)  [6][100/211]    eta: 0:00:21  time: 0.1925  data_time: 0.0384  memory: 2253  
2025/07/04 23:02:43 - mmengine - INFO - Epoch(val)  [6][150/211]    eta: 0:00:11  time: 0.1962  data_time: 0.0413  memory: 2253  
2025/07/04 23:02:53 - mmengine - INFO - Epoch(val)  [6][200/211]    eta: 0:00:02  time: 0.1927  data_time: 0.0389  memory: 2253  
2025/07/04 23:02:56 - mmengine - INFO - Evaluating bbox...
2025/07/04 23:03:02 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.347 | 0.824  | 0.204  | 0.342 | 0.588 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 23:03:02 - mmengine - INFO - bbox_mAP_copypaste: 0.347 0.824 0.204 0.342 0.588 -1.000
2025/07/04 23:03:02 - mmengine - INFO - Epoch(val) [6][211/211]    coco/UAV_precision: 0.3470  coco/bbox_mAP: 0.3470  coco/bbox_mAP_50: 0.8240  coco/bbox_mAP_75: 0.2040  coco/bbox_mAP_s: 0.3420  coco/bbox_mAP_m: 0.5880  coco/bbox_mAP_l: -1.0000  data_time: 0.0396  time: 0.1931
2025/07/04 23:03:40 - mmengine - INFO - Epoch(train)  [7][ 50/491]  lr: 1.0000e-02  eta: 0:36:56  time: 0.7551  data_time: 0.0390  memory: 18143  loss: 1.4972  loss_cls: 0.2494  loss_bbox: 0.5144  loss_dfl: 0.1589  backbone_loss_mask: 0.4189  clip_transfer_loss: 0.0807  kd_losses: 0.0750
2025/07/04 23:03:43 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 23:04:18 - mmengine - INFO - Epoch(train)  [7][100/491]  lr: 1.0000e-02  eta: 0:36:18  time: 0.7616  data_time: 0.0348  memory: 18143  loss: 1.4090  loss_cls: 0.2101  loss_bbox: 0.4958  loss_dfl: 0.1553  backbone_loss_mask: 0.4013  clip_transfer_loss: 0.0807  kd_losses: 0.0658
2025/07/04 23:04:55 - mmengine - INFO - Epoch(train)  [7][150/491]  lr: 1.0000e-02  eta: 0:35:39  time: 0.7562  data_time: 0.0389  memory: 18143  loss: 1.4111  loss_cls: 0.2124  loss_bbox: 0.5009  loss_dfl: 0.1578  backbone_loss_mask: 0.3936  clip_transfer_loss: 0.0798  kd_losses: 0.0667
2025/07/04 23:05:34 - mmengine - INFO - Epoch(train)  [7][200/491]  lr: 1.0000e-02  eta: 0:35:01  time: 0.7627  data_time: 0.0391  memory: 18143  loss: 1.3994  loss_cls: 0.2029  loss_bbox: 0.4937  loss_dfl: 0.1607  backbone_loss_mask: 0.3931  clip_transfer_loss: 0.0831  kd_losses: 0.0660
2025/07/04 23:06:11 - mmengine - INFO - Epoch(train)  [7][250/491]  lr: 1.0000e-02  eta: 0:34:22  time: 0.7575  data_time: 0.0363  memory: 18143  loss: 1.4064  loss_cls: 0.2161  loss_bbox: 0.5096  loss_dfl: 0.1603  backbone_loss_mask: 0.3822  clip_transfer_loss: 0.0740  kd_losses: 0.0642
2025/07/04 23:06:49 - mmengine - INFO - Epoch(train)  [7][300/491]  lr: 1.0000e-02  eta: 0:33:43  time: 0.7518  data_time: 0.0390  memory: 18143  loss: 1.4614  loss_cls: 0.2447  loss_bbox: 0.5288  loss_dfl: 0.1601  backbone_loss_mask: 0.3846  clip_transfer_loss: 0.0746  kd_losses: 0.0684
2025/07/04 23:07:27 - mmengine - INFO - Epoch(train)  [7][350/491]  lr: 1.0000e-02  eta: 0:33:05  time: 0.7580  data_time: 0.0390  memory: 18143  loss: 1.4011  loss_cls: 0.2173  loss_bbox: 0.5085  loss_dfl: 0.1577  backbone_loss_mask: 0.3715  clip_transfer_loss: 0.0781  kd_losses: 0.0680
2025/07/04 23:08:05 - mmengine - INFO - Epoch(train)  [7][400/491]  lr: 1.0000e-02  eta: 0:32:27  time: 0.7642  data_time: 0.0427  memory: 18143  loss: 1.3708  loss_cls: 0.2135  loss_bbox: 0.4939  loss_dfl: 0.1577  backbone_loss_mask: 0.3608  clip_transfer_loss: 0.0780  kd_losses: 0.0669
2025/07/04 23:08:43 - mmengine - INFO - Epoch(train)  [7][450/491]  lr: 1.0000e-02  eta: 0:31:48  time: 0.7642  data_time: 0.0399  memory: 18143  loss: 1.3968  loss_cls: 0.2292  loss_bbox: 0.5048  loss_dfl: 0.1590  backbone_loss_mask: 0.3639  clip_transfer_loss: 0.0754  kd_losses: 0.0646
2025/07/04 23:09:14 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 23:09:14 - mmengine - INFO - Saving checkpoint at 7 epochs
2025/07/04 23:09:26 - mmengine - INFO - Epoch(val)  [7][ 50/211]    eta: 0:00:31  time: 0.1952  data_time: 0.0408  memory: 18143  
2025/07/04 23:09:36 - mmengine - INFO - Epoch(val)  [7][100/211]    eta: 0:00:21  time: 0.1911  data_time: 0.0372  memory: 2253  
2025/07/04 23:09:46 - mmengine - INFO - Epoch(val)  [7][150/211]    eta: 0:00:11  time: 0.1932  data_time: 0.0386  memory: 2253  
2025/07/04 23:09:55 - mmengine - INFO - Epoch(val)  [7][200/211]    eta: 0:00:02  time: 0.1910  data_time: 0.0376  memory: 2253  
2025/07/04 23:09:58 - mmengine - INFO - Evaluating bbox...
2025/07/04 23:10:01 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.363 | 0.843  | 0.229  | 0.356 | 0.63  | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 23:10:01 - mmengine - INFO - bbox_mAP_copypaste: 0.363 0.843 0.229 0.356 0.630 -1.000
2025/07/04 23:10:01 - mmengine - INFO - Epoch(val) [7][211/211]    coco/UAV_precision: 0.3630  coco/bbox_mAP: 0.3630  coco/bbox_mAP_50: 0.8430  coco/bbox_mAP_75: 0.2290  coco/bbox_mAP_s: 0.3560  coco/bbox_mAP_m: 0.6300  coco/bbox_mAP_l: -1.0000  data_time: 0.0381  time: 0.1910
2025/07/04 23:10:39 - mmengine - INFO - Epoch(train)  [8][ 50/491]  lr: 1.0000e-02  eta: 0:30:38  time: 0.7553  data_time: 0.0393  memory: 18143  loss: 1.3291  loss_cls: 0.1925  loss_bbox: 0.4833  loss_dfl: 0.1570  backbone_loss_mask: 0.3565  clip_transfer_loss: 0.0759  kd_losses: 0.0639
2025/07/04 23:11:16 - mmengine - INFO - Epoch(train)  [8][100/491]  lr: 1.0000e-02  eta: 0:29:59  time: 0.7497  data_time: 0.0364  memory: 18143  loss: 1.3424  loss_cls: 0.2090  loss_bbox: 0.5033  loss_dfl: 0.1552  backbone_loss_mask: 0.3416  clip_transfer_loss: 0.0753  kd_losses: 0.0580
2025/07/04 23:11:53 - mmengine - INFO - Epoch(train)  [8][150/491]  lr: 1.0000e-02  eta: 0:29:20  time: 0.7433  data_time: 0.0308  memory: 18143  loss: 1.2662  loss_cls: 0.1630  loss_bbox: 0.4873  loss_dfl: 0.1543  backbone_loss_mask: 0.3350  clip_transfer_loss: 0.0713  kd_losses: 0.0554
2025/07/04 23:12:31 - mmengine - INFO - Epoch(train)  [8][200/491]  lr: 1.0000e-02  eta: 0:28:42  time: 0.7522  data_time: 0.0333  memory: 18143  loss: 1.3244  loss_cls: 0.2085  loss_bbox: 0.4912  loss_dfl: 0.1594  backbone_loss_mask: 0.3323  clip_transfer_loss: 0.0747  kd_losses: 0.0582
2025/07/04 23:13:08 - mmengine - INFO - Epoch(train)  [8][250/491]  lr: 1.0000e-02  eta: 0:28:03  time: 0.7529  data_time: 0.0289  memory: 18143  loss: 1.3214  loss_cls: 0.1982  loss_bbox: 0.4965  loss_dfl: 0.1569  backbone_loss_mask: 0.3357  clip_transfer_loss: 0.0755  kd_losses: 0.0586
2025/07/04 23:13:47 - mmengine - INFO - Epoch(train)  [8][300/491]  lr: 1.0000e-02  eta: 0:27:25  time: 0.7643  data_time: 0.0407  memory: 18143  loss: 1.2981  loss_cls: 0.1939  loss_bbox: 0.4853  loss_dfl: 0.1552  backbone_loss_mask: 0.3293  clip_transfer_loss: 0.0764  kd_losses: 0.0580
2025/07/04 23:14:24 - mmengine - INFO - Epoch(train)  [8][350/491]  lr: 1.0000e-02  eta: 0:26:47  time: 0.7461  data_time: 0.0363  memory: 18143  loss: 1.2935  loss_cls: 0.1938  loss_bbox: 0.4895  loss_dfl: 0.1551  backbone_loss_mask: 0.3231  clip_transfer_loss: 0.0757  kd_losses: 0.0563
2025/07/04 23:15:01 - mmengine - INFO - Epoch(train)  [8][400/491]  lr: 1.0000e-02  eta: 0:26:08  time: 0.7501  data_time: 0.0300  memory: 18143  loss: 1.2937  loss_cls: 0.1973  loss_bbox: 0.4901  loss_dfl: 0.1557  backbone_loss_mask: 0.3230  clip_transfer_loss: 0.0717  kd_losses: 0.0559
2025/07/04 23:15:39 - mmengine - INFO - Epoch(train)  [8][450/491]  lr: 1.0000e-02  eta: 0:25:30  time: 0.7574  data_time: 0.0341  memory: 18143  loss: 1.2891  loss_cls: 0.2051  loss_bbox: 0.4795  loss_dfl: 0.1588  backbone_loss_mask: 0.3187  clip_transfer_loss: 0.0676  kd_losses: 0.0595
2025/07/04 23:16:10 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 23:16:10 - mmengine - INFO - Saving checkpoint at 8 epochs
2025/07/04 23:16:22 - mmengine - INFO - Epoch(val)  [8][ 50/211]    eta: 0:00:31  time: 0.1940  data_time: 0.0389  memory: 18143  
2025/07/04 23:16:32 - mmengine - INFO - Epoch(val)  [8][100/211]    eta: 0:00:21  time: 0.1934  data_time: 0.0390  memory: 2253  
2025/07/04 23:16:41 - mmengine - INFO - Epoch(val)  [8][150/211]    eta: 0:00:11  time: 0.1903  data_time: 0.0363  memory: 2253  
2025/07/04 23:16:51 - mmengine - INFO - Epoch(val)  [8][200/211]    eta: 0:00:02  time: 0.1944  data_time: 0.0393  memory: 2253  
2025/07/04 23:16:54 - mmengine - INFO - Evaluating bbox...
2025/07/04 23:16:57 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.378 | 0.836  | 0.267  | 0.373 | 0.613 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 23:16:57 - mmengine - INFO - bbox_mAP_copypaste: 0.378 0.836 0.267 0.373 0.613 -1.000
2025/07/04 23:16:57 - mmengine - INFO - Epoch(val) [8][211/211]    coco/UAV_precision: 0.3780  coco/bbox_mAP: 0.3780  coco/bbox_mAP_50: 0.8360  coco/bbox_mAP_75: 0.2670  coco/bbox_mAP_s: 0.3730  coco/bbox_mAP_m: 0.6130  coco/bbox_mAP_l: -1.0000  data_time: 0.0380  time: 0.1915
2025/07/04 23:17:35 - mmengine - INFO - Epoch(train)  [9][ 50/491]  lr: 1.0000e-03  eta: 0:24:20  time: 0.7576  data_time: 0.0419  memory: 18143  loss: 1.2062  loss_cls: 0.1559  loss_bbox: 0.4656  loss_dfl: 0.1528  backbone_loss_mask: 0.3078  clip_transfer_loss: 0.0711  kd_losses: 0.0529
2025/07/04 23:17:51 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 23:18:11 - mmengine - INFO - Epoch(train)  [9][100/491]  lr: 1.0000e-03  eta: 0:23:41  time: 0.7374  data_time: 0.0345  memory: 18143  loss: 1.1923  loss_cls: 0.1598  loss_bbox: 0.4511  loss_dfl: 0.1527  backbone_loss_mask: 0.3106  clip_transfer_loss: 0.0720  kd_losses: 0.0462
2025/07/04 23:18:49 - mmengine - INFO - Epoch(train)  [9][150/491]  lr: 1.0000e-03  eta: 0:23:02  time: 0.7477  data_time: 0.0349  memory: 18143  loss: 1.1526  loss_cls: 0.1473  loss_bbox: 0.4250  loss_dfl: 0.1487  backbone_loss_mask: 0.3065  clip_transfer_loss: 0.0792  kd_losses: 0.0459
2025/07/04 23:19:26 - mmengine - INFO - Epoch(train)  [9][200/491]  lr: 1.0000e-03  eta: 0:22:24  time: 0.7377  data_time: 0.0286  memory: 18143  loss: 1.1579  loss_cls: 0.1497  loss_bbox: 0.4300  loss_dfl: 0.1492  backbone_loss_mask: 0.3151  clip_transfer_loss: 0.0691  kd_losses: 0.0449
2025/07/04 23:20:03 - mmengine - INFO - Epoch(train)  [9][250/491]  lr: 1.0000e-03  eta: 0:21:45  time: 0.7372  data_time: 0.0281  memory: 18143  loss: 1.1334  loss_cls: 0.1491  loss_bbox: 0.4257  loss_dfl: 0.1493  backbone_loss_mask: 0.3010  clip_transfer_loss: 0.0641  kd_losses: 0.0443
2025/07/04 23:20:39 - mmengine - INFO - Epoch(train)  [9][300/491]  lr: 1.0000e-03  eta: 0:21:07  time: 0.7387  data_time: 0.0265  memory: 18143  loss: 1.1384  loss_cls: 0.1606  loss_bbox: 0.4170  loss_dfl: 0.1507  backbone_loss_mask: 0.2972  clip_transfer_loss: 0.0672  kd_losses: 0.0457
2025/07/04 23:21:16 - mmengine - INFO - Epoch(train)  [9][350/491]  lr: 1.0000e-03  eta: 0:20:28  time: 0.7396  data_time: 0.0262  memory: 18143  loss: 1.1403  loss_cls: 0.1467  loss_bbox: 0.4229  loss_dfl: 0.1493  backbone_loss_mask: 0.3023  clip_transfer_loss: 0.0746  kd_losses: 0.0445
2025/07/04 23:21:54 - mmengine - INFO - Epoch(train)  [9][400/491]  lr: 1.0000e-03  eta: 0:19:50  time: 0.7521  data_time: 0.0366  memory: 18143  loss: 1.1595  loss_cls: 0.1577  loss_bbox: 0.4333  loss_dfl: 0.1507  backbone_loss_mask: 0.3065  clip_transfer_loss: 0.0681  kd_losses: 0.0432
2025/07/04 23:22:31 - mmengine - INFO - Epoch(train)  [9][450/491]  lr: 1.0000e-03  eta: 0:19:12  time: 0.7403  data_time: 0.0348  memory: 18143  loss: 1.1707  loss_cls: 0.1658  loss_bbox: 0.4292  loss_dfl: 0.1483  backbone_loss_mask: 0.3083  clip_transfer_loss: 0.0752  kd_losses: 0.0439
2025/07/04 23:23:01 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 23:23:01 - mmengine - INFO - Saving checkpoint at 9 epochs
2025/07/04 23:23:13 - mmengine - INFO - Epoch(val)  [9][ 50/211]    eta: 0:00:30  time: 0.1923  data_time: 0.0397  memory: 18143  
2025/07/04 23:23:23 - mmengine - INFO - Epoch(val)  [9][100/211]    eta: 0:00:20  time: 0.1854  data_time: 0.0331  memory: 2253  
2025/07/04 23:23:32 - mmengine - INFO - Epoch(val)  [9][150/211]    eta: 0:00:11  time: 0.1859  data_time: 0.0316  memory: 2253  
2025/07/04 23:23:41 - mmengine - INFO - Epoch(val)  [9][200/211]    eta: 0:00:02  time: 0.1854  data_time: 0.0324  memory: 2253  
2025/07/04 23:23:43 - mmengine - INFO - Evaluating bbox...
2025/07/04 23:23:46 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.429 | 0.883  | 0.348  | 0.424 | 0.641 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 23:23:46 - mmengine - INFO - bbox_mAP_copypaste: 0.429 0.883 0.348 0.424 0.641 -1.000
2025/07/04 23:23:46 - mmengine - INFO - Epoch(val) [9][211/211]    coco/UAV_precision: 0.4290  coco/bbox_mAP: 0.4290  coco/bbox_mAP_50: 0.8830  coco/bbox_mAP_75: 0.3480  coco/bbox_mAP_s: 0.4240  coco/bbox_mAP_m: 0.6410  coco/bbox_mAP_l: -1.0000  data_time: 0.0338  time: 0.1856
2025/07/04 23:24:24 - mmengine - INFO - Epoch(train) [10][ 50/491]  lr: 1.0000e-03  eta: 0:18:02  time: 0.7578  data_time: 0.0403  memory: 18143  loss: 1.1282  loss_cls: 0.1438  loss_bbox: 0.4196  loss_dfl: 0.1465  backbone_loss_mask: 0.3038  clip_transfer_loss: 0.0716  kd_losses: 0.0429
2025/07/04 23:25:02 - mmengine - INFO - Epoch(train) [10][100/491]  lr: 1.0000e-03  eta: 0:17:24  time: 0.7535  data_time: 0.0361  memory: 18143  loss: 1.1091  loss_cls: 0.1412  loss_bbox: 0.4036  loss_dfl: 0.1490  backbone_loss_mask: 0.2985  clip_transfer_loss: 0.0728  kd_losses: 0.0441
2025/07/04 23:25:39 - mmengine - INFO - Epoch(train) [10][150/491]  lr: 1.0000e-03  eta: 0:16:46  time: 0.7510  data_time: 0.0391  memory: 18143  loss: 1.1134  loss_cls: 0.1395  loss_bbox: 0.4072  loss_dfl: 0.1477  backbone_loss_mask: 0.3037  clip_transfer_loss: 0.0733  kd_losses: 0.0420
2025/07/04 23:26:17 - mmengine - INFO - Epoch(train) [10][200/491]  lr: 1.0000e-03  eta: 0:16:07  time: 0.7540  data_time: 0.0431  memory: 18143  loss: 1.1039  loss_cls: 0.1387  loss_bbox: 0.4045  loss_dfl: 0.1477  backbone_loss_mask: 0.2975  clip_transfer_loss: 0.0731  kd_losses: 0.0425
2025/07/04 23:26:54 - mmengine - INFO - Epoch(train) [10][250/491]  lr: 1.0000e-03  eta: 0:15:29  time: 0.7479  data_time: 0.0370  memory: 18143  loss: 1.1195  loss_cls: 0.1383  loss_bbox: 0.4238  loss_dfl: 0.1493  backbone_loss_mask: 0.3021  clip_transfer_loss: 0.0629  kd_losses: 0.0429
2025/07/04 23:27:32 - mmengine - INFO - Epoch(train) [10][300/491]  lr: 1.0000e-03  eta: 0:14:51  time: 0.7481  data_time: 0.0364  memory: 18143  loss: 1.1419  loss_cls: 0.1458  loss_bbox: 0.4327  loss_dfl: 0.1485  backbone_loss_mask: 0.3084  clip_transfer_loss: 0.0633  kd_losses: 0.0431
2025/07/04 23:28:09 - mmengine - INFO - Epoch(train) [10][350/491]  lr: 1.0000e-03  eta: 0:14:13  time: 0.7431  data_time: 0.0333  memory: 18143  loss: 1.1192  loss_cls: 0.1502  loss_bbox: 0.4067  loss_dfl: 0.1480  backbone_loss_mask: 0.3010  clip_transfer_loss: 0.0720  kd_losses: 0.0414
2025/07/04 23:28:46 - mmengine - INFO - Epoch(train) [10][400/491]  lr: 1.0000e-03  eta: 0:13:35  time: 0.7443  data_time: 0.0399  memory: 18143  loss: 1.1206  loss_cls: 0.1460  loss_bbox: 0.4118  loss_dfl: 0.1497  backbone_loss_mask: 0.2942  clip_transfer_loss: 0.0771  kd_losses: 0.0418
2025/07/04 23:29:24 - mmengine - INFO - Epoch(train) [10][450/491]  lr: 1.0000e-03  eta: 0:12:57  time: 0.7522  data_time: 0.0383  memory: 18143  loss: 1.1144  loss_cls: 0.1367  loss_bbox: 0.4151  loss_dfl: 0.1467  backbone_loss_mask: 0.3045  clip_transfer_loss: 0.0701  kd_losses: 0.0412
2025/07/04 23:29:54 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 23:29:54 - mmengine - INFO - Saving checkpoint at 10 epochs
2025/07/04 23:30:06 - mmengine - INFO - Epoch(val) [10][ 50/211]    eta: 0:00:31  time: 0.1982  data_time: 0.0440  memory: 18143  
2025/07/04 23:30:16 - mmengine - INFO - Epoch(val) [10][100/211]    eta: 0:00:21  time: 0.1914  data_time: 0.0383  memory: 2253  
2025/07/04 23:30:26 - mmengine - INFO - Epoch(val) [10][150/211]    eta: 0:00:11  time: 0.1931  data_time: 0.0396  memory: 2253  
2025/07/04 23:30:35 - mmengine - INFO - Epoch(val) [10][200/211]    eta: 0:00:02  time: 0.1919  data_time: 0.0387  memory: 2253  
2025/07/04 23:30:38 - mmengine - INFO - Evaluating bbox...
2025/07/04 23:30:40 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.428 | 0.885  | 0.348  | 0.422 | 0.641 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 23:30:40 - mmengine - INFO - bbox_mAP_copypaste: 0.428 0.885 0.348 0.422 0.641 -1.000
2025/07/04 23:30:40 - mmengine - INFO - Epoch(val) [10][211/211]    coco/UAV_precision: 0.4280  coco/bbox_mAP: 0.4280  coco/bbox_mAP_50: 0.8850  coco/bbox_mAP_75: 0.3480  coco/bbox_mAP_s: 0.4220  coco/bbox_mAP_m: 0.6410  coco/bbox_mAP_l: -1.0000  data_time: 0.0396  time: 0.1920
2025/07/04 23:31:17 - mmengine - INFO - Epoch(train) [11][ 50/491]  lr: 1.0000e-03  eta: 0:11:47  time: 0.7477  data_time: 0.0402  memory: 18143  loss: 1.1152  loss_cls: 0.1416  loss_bbox: 0.4097  loss_dfl: 0.1467  backbone_loss_mask: 0.3050  clip_transfer_loss: 0.0712  kd_losses: 0.0409
2025/07/04 23:31:47 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 23:31:54 - mmengine - INFO - Epoch(train) [11][100/491]  lr: 1.0000e-03  eta: 0:11:09  time: 0.7471  data_time: 0.0361  memory: 18143  loss: 1.0912  loss_cls: 0.1423  loss_bbox: 0.3997  loss_dfl: 0.1475  backbone_loss_mask: 0.2932  clip_transfer_loss: 0.0672  kd_losses: 0.0411
2025/07/04 23:32:33 - mmengine - INFO - Epoch(train) [11][150/491]  lr: 1.0000e-03  eta: 0:10:31  time: 0.7704  data_time: 0.0394  memory: 18143  loss: 1.1194  loss_cls: 0.1422  loss_bbox: 0.4151  loss_dfl: 0.1469  backbone_loss_mask: 0.3032  clip_transfer_loss: 0.0686  kd_losses: 0.0436
2025/07/04 23:34:23 - mmengine - INFO - Epoch(train) [11][200/491]  lr: 1.0000e-03  eta: 0:10:04  time: 2.1980  data_time: 0.0819  memory: 18143  loss: 1.1015  loss_cls: 0.1359  loss_bbox: 0.4019  loss_dfl: 0.1466  backbone_loss_mask: 0.3042  clip_transfer_loss: 0.0702  kd_losses: 0.0427
2025/07/04 23:36:08 - mmengine - INFO - Epoch(train) [11][250/491]  lr: 1.0000e-03  eta: 0:09:35  time: 2.1002  data_time: 0.0724  memory: 18143  loss: 1.0893  loss_cls: 0.1332  loss_bbox: 0.4028  loss_dfl: 0.1489  backbone_loss_mask: 0.2921  clip_transfer_loss: 0.0720  kd_losses: 0.0402
2025/07/04 23:37:57 - mmengine - INFO - Epoch(train) [11][300/491]  lr: 1.0000e-03  eta: 0:09:05  time: 2.1817  data_time: 0.0768  memory: 18143  loss: 1.0925  loss_cls: 0.1333  loss_bbox: 0.4129  loss_dfl: 0.1446  backbone_loss_mask: 0.2960  clip_transfer_loss: 0.0642  kd_losses: 0.0415
2025/07/04 23:39:50 - mmengine - INFO - Epoch(train) [11][350/491]  lr: 1.0000e-03  eta: 0:08:34  time: 2.2540  data_time: 0.0758  memory: 18143  loss: 1.0983  loss_cls: 0.1417  loss_bbox: 0.4052  loss_dfl: 0.1479  backbone_loss_mask: 0.2931  clip_transfer_loss: 0.0693  kd_losses: 0.0411
2025/07/04 23:41:39 - mmengine - INFO - Epoch(train) [11][400/491]  lr: 1.0000e-03  eta: 0:08:01  time: 2.1840  data_time: 0.0785  memory: 18143  loss: 1.0861  loss_cls: 0.1278  loss_bbox: 0.4055  loss_dfl: 0.1489  backbone_loss_mask: 0.2953  clip_transfer_loss: 0.0677  kd_losses: 0.0409
2025/07/04 23:43:35 - mmengine - INFO - Epoch(train) [11][450/491]  lr: 1.0000e-03  eta: 0:07:27  time: 2.3220  data_time: 0.0797  memory: 18143  loss: 1.0945  loss_cls: 0.1331  loss_bbox: 0.4002  loss_dfl: 0.1481  backbone_loss_mask: 0.2950  clip_transfer_loss: 0.0763  kd_losses: 0.0418
2025/07/04 23:45:02 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/04 23:45:02 - mmengine - INFO - Saving checkpoint at 11 epochs
2025/07/04 23:45:33 - mmengine - INFO - Epoch(val) [11][ 50/211]    eta: 0:01:24  time: 0.5234  data_time: 0.0988  memory: 18143  
2025/07/04 23:45:58 - mmengine - INFO - Epoch(val) [11][100/211]    eta: 0:00:57  time: 0.5083  data_time: 0.0806  memory: 2253  
2025/07/04 23:46:23 - mmengine - INFO - Epoch(val) [11][150/211]    eta: 0:00:30  time: 0.4856  data_time: 0.0748  memory: 2253  
2025/07/04 23:46:49 - mmengine - INFO - Epoch(val) [11][200/211]    eta: 0:00:05  time: 0.5193  data_time: 0.0899  memory: 2253  
2025/07/04 23:46:54 - mmengine - INFO - Evaluating bbox...
2025/07/04 23:46:58 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.431 | 0.885  | 0.351  | 0.425 | 0.648 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 23:46:58 - mmengine - INFO - bbox_mAP_copypaste: 0.431 0.885 0.351 0.425 0.648 -1.000
2025/07/04 23:46:58 - mmengine - INFO - Epoch(val) [11][211/211]    coco/UAV_precision: 0.4310  coco/bbox_mAP: 0.4310  coco/bbox_mAP_50: 0.8850  coco/bbox_mAP_75: 0.3510  coco/bbox_mAP_s: 0.4250  coco/bbox_mAP_m: 0.6480  coco/bbox_mAP_l: -1.0000  data_time: 0.0841  time: 0.5027
2025/07/04 23:48:47 - mmengine - INFO - Epoch(train) [12][ 50/491]  lr: 1.0000e-04  eta: 0:06:20  time: 2.1736  data_time: 0.0846  memory: 18143  loss: 1.0699  loss_cls: 0.1214  loss_bbox: 0.3943  loss_dfl: 0.1444  backbone_loss_mask: 0.2955  clip_transfer_loss: 0.0730  kd_losses: 0.0413
2025/07/04 23:50:28 - mmengine - INFO - Epoch(train) [12][100/491]  lr: 1.0000e-04  eta: 0:05:41  time: 2.0260  data_time: 0.0816  memory: 18143  loss: 1.0619  loss_cls: 0.1222  loss_bbox: 0.3850  loss_dfl: 0.1467  backbone_loss_mask: 0.2920  clip_transfer_loss: 0.0754  kd_losses: 0.0406
2025/07/04 23:52:20 - mmengine - INFO - Epoch(train) [12][150/491]  lr: 1.0000e-04  eta: 0:05:01  time: 2.2460  data_time: 0.0824  memory: 18143  loss: 1.0874  loss_cls: 0.1324  loss_bbox: 0.3987  loss_dfl: 0.1453  backbone_loss_mask: 0.2985  clip_transfer_loss: 0.0719  kd_losses: 0.0406
2025/07/04 23:54:08 - mmengine - INFO - Epoch(train) [12][200/491]  lr: 1.0000e-04  eta: 0:04:20  time: 2.1599  data_time: 0.0756  memory: 18143  loss: 1.0956  loss_cls: 0.1350  loss_bbox: 0.4034  loss_dfl: 0.1470  backbone_loss_mask: 0.2991  clip_transfer_loss: 0.0707  kd_losses: 0.0403
2025/07/04 23:55:58 - mmengine - INFO - Epoch(train) [12][250/491]  lr: 1.0000e-04  eta: 0:03:38  time: 2.2021  data_time: 0.0775  memory: 18143  loss: 1.0994  loss_cls: 0.1388  loss_bbox: 0.4051  loss_dfl: 0.1483  backbone_loss_mask: 0.2973  clip_transfer_loss: 0.0682  kd_losses: 0.0418
2025/07/04 23:57:42 - mmengine - INFO - Epoch(train) [12][300/491]  lr: 1.0000e-04  eta: 0:02:55  time: 2.0762  data_time: 0.0874  memory: 18143  loss: 1.0836  loss_cls: 0.1304  loss_bbox: 0.3955  loss_dfl: 0.1459  backbone_loss_mask: 0.2985  clip_transfer_loss: 0.0729  kd_losses: 0.0404
2025/07/04 23:59:30 - mmengine - INFO - Epoch(train) [12][350/491]  lr: 1.0000e-04  eta: 0:02:11  time: 2.1563  data_time: 0.0780  memory: 18143  loss: 1.0906  loss_cls: 0.1438  loss_bbox: 0.3907  loss_dfl: 0.1455  backbone_loss_mask: 0.3001  clip_transfer_loss: 0.0691  kd_losses: 0.0414
2025/07/05 00:01:14 - mmengine - INFO - Epoch(train) [12][400/491]  lr: 1.0000e-04  eta: 0:01:25  time: 2.0713  data_time: 0.0795  memory: 18143  loss: 1.0705  loss_cls: 0.1272  loss_bbox: 0.3928  loss_dfl: 0.1482  backbone_loss_mask: 0.2938  clip_transfer_loss: 0.0682  kd_losses: 0.0403
2025/07/05 00:03:06 - mmengine - INFO - Epoch(train) [12][450/491]  lr: 1.0000e-04  eta: 0:00:38  time: 2.2400  data_time: 0.0743  memory: 18143  loss: 1.0585  loss_cls: 0.1211  loss_bbox: 0.3842  loss_dfl: 0.1445  backbone_loss_mask: 0.2948  clip_transfer_loss: 0.0731  kd_losses: 0.0409
2025/07/05 00:04:29 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_kdMed2Ear_20250704_221954
2025/07/05 00:04:30 - mmengine - INFO - Saving checkpoint at 12 epochs
2025/07/05 00:05:01 - mmengine - INFO - Epoch(val) [12][ 50/211]    eta: 0:01:24  time: 0.5264  data_time: 0.0912  memory: 18143  
2025/07/05 00:05:28 - mmengine - INFO - Epoch(val) [12][100/211]    eta: 0:00:58  time: 0.5305  data_time: 0.0773  memory: 2253  
2025/07/05 00:05:53 - mmengine - INFO - Epoch(val) [12][150/211]    eta: 0:00:31  time: 0.5086  data_time: 0.0926  memory: 2253  
2025/07/05 00:06:18 - mmengine - INFO - Epoch(val) [12][200/211]    eta: 0:00:05  time: 0.5043  data_time: 0.0793  memory: 2253  
2025/07/05 00:06:24 - mmengine - INFO - Evaluating bbox...
2025/07/05 00:06:28 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.433 | 0.886  | 0.354  | 0.428 | 0.653 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/05 00:06:28 - mmengine - INFO - bbox_mAP_copypaste: 0.433 0.886 0.354 0.428 0.653 -1.000
2025/07/05 00:06:28 - mmengine - INFO - Epoch(val) [12][211/211]    coco/UAV_precision: 0.4330  coco/bbox_mAP: 0.4330  coco/bbox_mAP_50: 0.8860  coco/bbox_mAP_75: 0.3540  coco/bbox_mAP_s: 0.4280  coco/bbox_mAP_m: 0.6530  coco/bbox_mAP_l: -1.0000  data_time: 0.0837  time: 0.5117
