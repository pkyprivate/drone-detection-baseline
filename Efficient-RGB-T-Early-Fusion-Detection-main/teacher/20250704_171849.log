2025/07/04 17:18:50 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 30490864
    GPU 0: NVIDIA A100-SXM4-40GB
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 11.8, V11.8.89
    GCC: gcc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0
    PyTorch: 2.7.1+cu118
    PyTorch compiling details: PyTorch built with:
  - GCC 11.2
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2024.2-Product Build 20240605 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.8
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_90,code=sm_90
  - CuDNN 90.1
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=e2d141dbde55c2a4370fac5165b0561b6af4798b, CUDA_VERSION=11.8, CUDNN_VERSION=9.1.0, CXX_COMPILER=/opt/rh/gcc-toolset-11/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=range-loop-construct -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-unknown-pragmas -Wno-unused-parameter -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.7.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.22.1+cu118
    OpenCV: 4.11.0
    MMEngine: 0.10.7

Runtime environment:
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: 30490864
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2025/07/04 17:18:51 - mmengine - INFO - Config:
auto_scale_lr = dict(base_batch_size=16, enable=False)
backend_args = None
data_root = '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV'
dataset_type = 'M3FDDataset'
default_hooks = dict(
    checkpoint=dict(interval=1, type='CheckpointHook'),
    logger=dict(interval=50, type='LoggerHook'),
    param_scheduler=dict(type='ParamSchedulerHook'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    timer=dict(type='IterTimerHook'),
    visualization=dict(type='DetVisualizationHook'))
default_scope = 'mmdet'
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
launcher = 'none'
load_from = 'https://download.openmmlab.com/mmdetection/v2.0/gfl/gfl_r50_fpn_1x_coco/gfl_r50_fpn_1x_coco_20200629_121244-25944287.pth'
log_level = 'INFO'
log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)
model = dict(
    backbone=dict(
        depth=50,
        frozen_stages=-1,
        in_channels=3,
        init_cfg=dict(checkpoint='torchvision://resnet50', type='Pretrained'),
        norm_cfg=dict(requires_grad=True, type='BN'),
        norm_eval=True,
        num_stages=4,
        out_indices=(
            0,
            1,
            2,
            3,
        ),
        style='pytorch',
        type='ResNetRGBTEarlyModifiedStem'),
    bbox_head=dict(
        anchor_generator=dict(
            octave_base_scale=8,
            ratios=[
                1.0,
            ],
            scales_per_octave=1,
            strides=[
                8,
                16,
                32,
                64,
                128,
            ],
            type='AnchorGenerator'),
        feat_channels=256,
        in_channels=256,
        loss_bbox=dict(loss_weight=2.0, type='GIoULoss'),
        loss_cls=dict(
            beta=2.0,
            loss_weight=1.0,
            type='QualityFocalLoss',
            use_sigmoid=True),
        loss_dfl=dict(loss_weight=0.25, type='DistributionFocalLoss'),
        num_classes=6,
        reg_max=16,
        stacked_convs=4,
        type='GFLHead'),
    data_preprocessor=dict(
        bgr_to_rgb=True,
        pad_size_divisor=32,
        rgb_mean=[
            128.2,
            129.3,
            125.3,
        ],
        rgb_std=[
            49.1,
            50.2,
            53.5,
        ],
        thermal_mean=[
            84.1,
            84.1,
            84.1,
        ],
        thermal_std=[
            50.6,
            50.6,
            50.6,
        ],
        type='RGBTDetDataPreprocessor'),
    neck=dict(
        add_extra_convs='on_output',
        in_channels=[
            256,
            512,
            1024,
            2048,
        ],
        num_outs=5,
        out_channels=256,
        start_level=1,
        type='FPN'),
    test_cfg=dict(
        max_per_img=100,
        min_bbox_size=0,
        nms=dict(iou_threshold=0.6, type='nms'),
        nms_pre=1000,
        score_thr=0.05),
    train_cfg=dict(
        allowed_border=-1,
        assigner=dict(topk=9, type='ATSSAssigner'),
        debug=False,
        pos_weight=-1),
    type='GFLCLIP')
optim_wrapper = dict(
    loss_scale='dynamic',
    optimizer=dict(lr=0.01, momentum=0.9, type='SGD', weight_decay=0.0001),
    type='AmpOptimWrapper')
param_scheduler = [
    dict(begin=0, by_epoch=False, end=500, start_factor=0.01, type='LinearLR'),
    dict(
        begin=0,
        by_epoch=True,
        end=12,
        gamma=0.1,
        milestones=[
            8,
            11,
        ],
        type='MultiStepLR'),
]
resume = False
test_cfg = dict(type='TestLoop')
test_dataloader = dict(
    batch_size=16,
    dataset=dict(
        ann_file=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV/coco/infrared_test.json',
        backend_args=None,
        data_prefix=dict(img=''),
        data_root=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV',
        pipeline=[
            dict(backend_args=None, type='LoadRGBTImageFromFile'),
            dict(keep_ratio=True, scale=(
                640,
                512,
            ), type='ResizeRGBT'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetRGBTInputs'),
        ],
        test_mode=True,
        type='M3FDDataset'),
    drop_last=False,
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
test_evaluator = dict(
    ann_file=
    '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV/coco/infrared_test.json',
    backend_args=None,
    classwise=True,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
test_pipeline = [
    dict(backend_args=None, type='LoadRGBTImageFromFile'),
    dict(keep_ratio=True, scale=(
        640,
        512,
    ), type='ResizeRGBT'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(
        meta_keys=(
            'img_id',
            'img_path',
            'ori_shape',
            'img_shape',
            'scale_factor',
        ),
        type='PackDetRGBTInputs'),
]
train_cfg = dict(max_epochs=12, type='EpochBasedTrainLoop', val_interval=1)
train_dataloader = dict(
    batch_sampler=dict(type='AspectRatioBatchSampler'),
    batch_size=16,
    dataset=dict(
        ann_file=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV/coco/infrared_train.json',
        backend_args=None,
        data_prefix=dict(img=''),
        data_root=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV',
        filter_cfg=dict(filter_empty_gt=True, min_size=5),
        pipeline=[
            dict(backend_args=None, type='LoadRGBTImageFromFile'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(keep_ratio=True, scale=(
                640,
                512,
            ), type='ResizeRGBT'),
            dict(prob=0.5, type='RandomFlipRGBT'),
            dict(type='PackDetRGBTInputs'),
        ],
        type='M3FDDataset'),
    num_workers=2,
    persistent_workers=True,
    sampler=dict(shuffle=True, type='DefaultSampler'))
train_pipeline = [
    dict(backend_args=None, type='LoadRGBTImageFromFile'),
    dict(type='LoadAnnotations', with_bbox=True),
    dict(keep_ratio=True, scale=(
        640,
        512,
    ), type='ResizeRGBT'),
    dict(prob=0.5, type='RandomFlipRGBT'),
    dict(type='PackDetRGBTInputs'),
]
val_cfg = dict(type='ValLoop')
val_dataloader = dict(
    batch_size=16,
    dataset=dict(
        ann_file=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV/coco/infrared_test.json',
        backend_args=None,
        data_prefix=dict(img=''),
        data_root=
        '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV',
        pipeline=[
            dict(backend_args=None, type='LoadRGBTImageFromFile'),
            dict(keep_ratio=True, scale=(
                640,
                512,
            ), type='ResizeRGBT'),
            dict(type='LoadAnnotations', with_bbox=True),
            dict(
                meta_keys=(
                    'img_id',
                    'img_path',
                    'ori_shape',
                    'img_shape',
                    'scale_factor',
                ),
                type='PackDetRGBTInputs'),
        ],
        test_mode=True,
        type='M3FDDataset'),
    drop_last=False,
    num_workers=4,
    persistent_workers=True,
    sampler=dict(shuffle=False, type='DefaultSampler'))
val_evaluator = dict(
    ann_file=
    '/root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/UAV/coco/infrared_test.json',
    backend_args=None,
    classwise=True,
    format_only=False,
    metric='bbox',
    type='CocoMetric')
vis_backends = [
    dict(type='LocalVisBackend'),
]
visualizer = dict(
    name='visualizer',
    type='DetLocalVisualizer',
    vis_backends=[
        dict(type='LocalVisBackend'),
    ])
work_dir = './work_dirs/exp1'

2025/07/04 17:18:58 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2025/07/04 17:18:58 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DetVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2025/07/04 17:19:01 - mmengine - INFO - load model from: torchvision://resnet50
2025/07/04 17:19:01 - mmengine - INFO - Loads checkpoint by torchvision backend from path: torchvision://resnet50
2025/07/04 17:19:01 - mmengine - WARNING - The model and loaded state dict do not match exactly

unexpected key in source state_dict: fc.weight, fc.bias

missing keys in source state_dict: stem.conv.weight

Name of parameter - Initialization information

backbone.conv1.weight - torch.Size([64, 3, 7, 7]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv1.weight - torch.Size([64, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.0.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.0.downsample.1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.1.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv1.weight - torch.Size([64, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn1.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv2.weight - torch.Size([64, 64, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.weight - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn2.bias - torch.Size([64]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.conv3.weight - torch.Size([256, 64, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer1.2.bn3.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv1.weight - torch.Size([128, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.0.weight - torch.Size([512, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.0.downsample.1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.1.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.2.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv1.weight - torch.Size([128, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn1.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv2.weight - torch.Size([128, 128, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.weight - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn2.bias - torch.Size([128]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.conv3.weight - torch.Size([512, 128, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer2.3.bn3.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv1.weight - torch.Size([256, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.0.weight - torch.Size([1024, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.0.downsample.1.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.1.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.2.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.3.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.4.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv1.weight - torch.Size([256, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn1.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv2.weight - torch.Size([256, 256, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.weight - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn2.bias - torch.Size([256]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.conv3.weight - torch.Size([1024, 256, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.weight - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer3.5.bn3.bias - torch.Size([1024]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv1.weight - torch.Size([512, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.0.weight - torch.Size([2048, 1024, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.0.downsample.1.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.1.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv1.weight - torch.Size([512, 2048, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn1.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv2.weight - torch.Size([512, 512, 3, 3]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.weight - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn2.bias - torch.Size([512]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.conv3.weight - torch.Size([2048, 512, 1, 1]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.weight - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.layer4.2.bn3.bias - torch.Size([2048]): 
PretrainedInit: load from torchvision://resnet50 

backbone.stem.conv.weight - torch.Size([3, 6, 7, 7]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

neck.lateral_convs.0.conv.weight - torch.Size([256, 512, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

neck.lateral_convs.1.conv.weight - torch.Size([256, 1024, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

neck.lateral_convs.2.conv.weight - torch.Size([256, 2048, 1, 1]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.lateral_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.3.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

neck.fpn_convs.4.conv.weight - torch.Size([256, 256, 3, 3]): 
XavierInit: gain=1, distribution=uniform, bias=0 

neck.fpn_convs.4.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.cls_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.cls_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.cls_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.cls_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.cls_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.cls_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.cls_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.cls_convs.3.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.cls_convs.3.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.reg_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.reg_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.reg_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.reg_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.reg_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.reg_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.reg_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.reg_convs.3.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.reg_convs.3.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.gfl_cls.weight - torch.Size([6, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

bbox_head.gfl_cls.bias - torch.Size([6]): 
NormalInit: mean=0, std=0.01, bias=-4.59511985013459 

bbox_head.gfl_reg.weight - torch.Size([68, 256, 3, 3]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.gfl_reg.bias - torch.Size([68]): 
NormalInit: mean=0, std=0.01, bias=0 

bbox_head.scales.0.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.scales.1.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.scales.2.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.scales.3.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

bbox_head.scales.4.scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

match_convs.0.weight - torch.Size([256, 512, 1, 1]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

match_convs.1.weight - torch.Size([256, 1024, 1, 1]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

match_convs.2.weight - torch.Size([256, 2048, 1, 1]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

resnet_fc.weight - torch.Size([6, 256]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

resnet_mask_conv.0.weight - torch.Size([6, 256, 1, 1]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

resnet_mask_conv.1.weight - torch.Size([6]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

resnet_mask_conv.1.bias - torch.Size([6]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

resnet_mask_conv.3.weight - torch.Size([6, 6, 3, 3]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

resnet_mask_conv.4.weight - torch.Size([6]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

resnet_mask_conv.4.bias - torch.Size([6]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.positional_embedding - torch.Size([77, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.text_projection - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.logit_scale - torch.Size([]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.class_embedding - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.positional_embedding - torch.Size([50, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.proj - torch.Size([768, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.conv1.weight - torch.Size([768, 3, 32, 32]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.ln_pre.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.ln_pre.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.0.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.1.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.2.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.3.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.4.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.5.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.6.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.7.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.8.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.9.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.10.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.attn.in_proj_weight - torch.Size([2304, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.attn.in_proj_bias - torch.Size([2304]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.attn.out_proj.weight - torch.Size([768, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.attn.out_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.ln_1.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.ln_1.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight - torch.Size([3072, 768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias - torch.Size([3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight - torch.Size([768, 3072]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.ln_2.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.transformer.resblocks.11.ln_2.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.ln_post.weight - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.visual.ln_post.bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.0.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.1.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.2.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.3.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.4.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.5.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.6.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.7.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.8.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.9.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.10.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.attn.in_proj_weight - torch.Size([1536, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.attn.in_proj_bias - torch.Size([1536]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.attn.out_proj.weight - torch.Size([512, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.attn.out_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.ln_1.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.ln_1.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.mlp.c_fc.weight - torch.Size([2048, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.mlp.c_fc.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.mlp.c_proj.weight - torch.Size([512, 2048]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.mlp.c_proj.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.ln_2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.transformer.resblocks.11.ln_2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.token_embedding.weight - torch.Size([49408, 512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.ln_final.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_model.ln_final.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_fc.0.weight - torch.Size([3, 6]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_fc.1.weight - torch.Size([3]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_fc.1.bias - torch.Size([3]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_fc.3.weight - torch.Size([6, 3]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_fc.4.weight - torch.Size([6]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_fc.4.bias - torch.Size([6]): 
The value is the same before and after calling `init_weights` of GFLCLIP  

clip_fc.7.weight - torch.Size([6, 6]): 
The value is the same before and after calling `init_weights` of GFLCLIP  
2025/07/04 17:19:01 - mmengine - INFO - Load checkpoint from https://download.openmmlab.com/mmdetection/v2.0/gfl/gfl_r50_fpn_1x_coco/gfl_r50_fpn_1x_coco_20200629_121244-25944287.pth
2025/07/04 17:19:01 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2025/07/04 17:19:01 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2025/07/04 17:19:01 - mmengine - INFO - Checkpoints will be saved to /root/workspace/personal_data/pky/RGBT/Efficient-RGB-T-Early-Fusion-Detection-main/mmdetection/work_dirs/exp1.
2025/07/04 17:19:40 - mmengine - INFO - Epoch(train)  [1][ 50/491]  lr: 1.0721e-03  eta: 1:16:45  time: 0.7884  data_time: 0.0508  memory: 12672  loss: 5.1604  loss_cls: 2.4482  loss_bbox: 0.8245  loss_dfl: 0.2335  backbone_loss_mask: 0.9000  clip_transfer_loss: 0.7542
2025/07/04 17:20:14 - mmengine - INFO - Epoch(train)  [1][100/491]  lr: 2.0641e-03  eta: 1:10:17  time: 0.6678  data_time: 0.0405  memory: 12672  loss: 3.7325  loss_cls: 1.2540  loss_bbox: 0.6704  loss_dfl: 0.1879  backbone_loss_mask: 0.8928  clip_transfer_loss: 0.7275
2025/07/04 17:20:47 - mmengine - INFO - Epoch(train)  [1][150/491]  lr: 3.0561e-03  eta: 1:07:26  time: 0.6581  data_time: 0.0425  memory: 12672  loss: 3.2424  loss_cls: 0.8390  loss_bbox: 0.6561  loss_dfl: 0.1859  backbone_loss_mask: 0.8792  clip_transfer_loss: 0.6822
2025/07/04 17:21:20 - mmengine - INFO - Epoch(train)  [1][200/491]  lr: 4.0481e-03  eta: 1:06:02  time: 0.6700  data_time: 0.0386  memory: 12672  loss: 3.2938  loss_cls: 0.9387  loss_bbox: 0.6992  loss_dfl: 0.1884  backbone_loss_mask: 0.8610  clip_transfer_loss: 0.6065
2025/07/04 17:21:54 - mmengine - INFO - Epoch(train)  [1][250/491]  lr: 5.0401e-03  eta: 1:04:57  time: 0.6701  data_time: 0.0348  memory: 12672  loss: 3.0726  loss_cls: 0.9073  loss_bbox: 0.6853  loss_dfl: 0.1860  backbone_loss_mask: 0.8393  clip_transfer_loss: 0.4547
2025/07/04 17:22:28 - mmengine - INFO - Epoch(train)  [1][300/491]  lr: 6.0321e-03  eta: 1:04:11  time: 0.6777  data_time: 0.0409  memory: 12672  loss: 2.7350  loss_cls: 0.7065  loss_bbox: 0.6630  loss_dfl: 0.1822  backbone_loss_mask: 0.8138  clip_transfer_loss: 0.3694
2025/07/04 17:23:02 - mmengine - INFO - Epoch(train)  [1][350/491]  lr: 7.0240e-03  eta: 1:03:29  time: 0.6800  data_time: 0.0453  memory: 12672  loss: 2.7561  loss_cls: 0.7611  loss_bbox: 0.7120  loss_dfl: 0.1955  backbone_loss_mask: 0.7871  clip_transfer_loss: 0.3004
2025/07/04 17:23:36 - mmengine - INFO - Epoch(train)  [1][400/491]  lr: 8.0160e-03  eta: 1:02:51  time: 0.6817  data_time: 0.0426  memory: 12672  loss: 2.6244  loss_cls: 0.7186  loss_bbox: 0.6977  loss_dfl: 0.1849  backbone_loss_mask: 0.7600  clip_transfer_loss: 0.2633
2025/07/04 17:24:09 - mmengine - INFO - Epoch(train)  [1][450/491]  lr: 9.0080e-03  eta: 1:02:09  time: 0.6743  data_time: 0.0417  memory: 12672  loss: 2.5245  loss_cls: 0.6678  loss_bbox: 0.7053  loss_dfl: 0.1861  backbone_loss_mask: 0.7323  clip_transfer_loss: 0.2329
2025/07/04 17:24:38 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 17:24:38 - mmengine - INFO - Saving checkpoint at 1 epochs
2025/07/04 17:24:51 - mmengine - INFO - Epoch(val)  [1][ 50/211]    eta: 0:00:34  time: 0.2115  data_time: 0.0474  memory: 12672  
2025/07/04 17:25:00 - mmengine - INFO - Epoch(val)  [1][100/211]    eta: 0:00:22  time: 0.1913  data_time: 0.0360  memory: 1782  
2025/07/04 17:25:10 - mmengine - INFO - Epoch(val)  [1][150/211]    eta: 0:00:12  time: 0.1894  data_time: 0.0341  memory: 1782  
2025/07/04 17:25:19 - mmengine - INFO - Epoch(val)  [1][200/211]    eta: 0:00:02  time: 0.1916  data_time: 0.0341  memory: 1782  
2025/07/04 17:25:25 - mmengine - INFO - Evaluating bbox...
2025/07/04 17:25:38 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.173 | 0.508  | 0.063  | 0.173 | 0.181 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 17:25:38 - mmengine - INFO - bbox_mAP_copypaste: 0.173 0.508 0.063 0.173 0.181 -1.000
2025/07/04 17:25:38 - mmengine - INFO - Epoch(val) [1][211/211]    coco/UAV_precision: 0.1730  coco/bbox_mAP: 0.1730  coco/bbox_mAP_50: 0.5080  coco/bbox_mAP_75: 0.0630  coco/bbox_mAP_s: 0.1730  coco/bbox_mAP_m: 0.1810  coco/bbox_mAP_l: -1.0000  data_time: 0.0375  time: 0.1949
2025/07/04 17:26:11 - mmengine - INFO - Epoch(train)  [2][ 50/491]  lr: 1.0000e-02  eta: 1:01:02  time: 0.6624  data_time: 0.0383  memory: 12672  loss: 2.6130  loss_cls: 0.8120  loss_bbox: 0.7122  loss_dfl: 0.1891  backbone_loss_mask: 0.6880  clip_transfer_loss: 0.2118
2025/07/04 17:26:42 - mmengine - INFO - Epoch(train)  [2][100/491]  lr: 1.0000e-02  eta: 1:00:01  time: 0.6249  data_time: 0.0367  memory: 12672  loss: 2.3980  loss_cls: 0.6844  loss_bbox: 0.6650  loss_dfl: 0.1756  backbone_loss_mask: 0.6671  clip_transfer_loss: 0.2059
2025/07/04 17:27:12 - mmengine - INFO - Epoch(train)  [2][150/491]  lr: 1.0000e-02  eta: 0:58:55  time: 0.6008  data_time: 0.0372  memory: 12672  loss: 2.3322  loss_cls: 0.6160  loss_bbox: 0.6750  loss_dfl: 0.1881  backbone_loss_mask: 0.6505  clip_transfer_loss: 0.2026
2025/07/04 17:27:43 - mmengine - INFO - Epoch(train)  [2][200/491]  lr: 1.0000e-02  eta: 0:57:56  time: 0.6051  data_time: 0.0371  memory: 12672  loss: 2.3238  loss_cls: 0.6180  loss_bbox: 0.6691  loss_dfl: 0.1811  backbone_loss_mask: 0.6366  clip_transfer_loss: 0.2190
2025/07/04 17:28:13 - mmengine - INFO - Epoch(train)  [2][250/491]  lr: 1.0000e-02  eta: 0:57:02  time: 0.6103  data_time: 0.0359  memory: 12672  loss: 2.1877  loss_cls: 0.5528  loss_bbox: 0.6350  loss_dfl: 0.1751  backbone_loss_mask: 0.6247  clip_transfer_loss: 0.2002
2025/07/04 17:28:43 - mmengine - INFO - Epoch(train)  [2][300/491]  lr: 1.0000e-02  eta: 0:56:09  time: 0.6021  data_time: 0.0367  memory: 12672  loss: 2.2053  loss_cls: 0.5573  loss_bbox: 0.6454  loss_dfl: 0.1818  backbone_loss_mask: 0.6145  clip_transfer_loss: 0.2063
2025/07/04 17:29:13 - mmengine - INFO - Epoch(train)  [2][350/491]  lr: 1.0000e-02  eta: 0:55:17  time: 0.5974  data_time: 0.0353  memory: 12672  loss: 2.2621  loss_cls: 0.5710  loss_bbox: 0.6815  loss_dfl: 0.1891  backbone_loss_mask: 0.6055  clip_transfer_loss: 0.2151
2025/07/04 17:29:43 - mmengine - INFO - Epoch(train)  [2][400/491]  lr: 1.0000e-02  eta: 0:54:27  time: 0.5950  data_time: 0.0327  memory: 12672  loss: 2.0912  loss_cls: 0.4841  loss_bbox: 0.6530  loss_dfl: 0.1764  backbone_loss_mask: 0.5976  clip_transfer_loss: 0.1802
2025/07/04 17:30:13 - mmengine - INFO - Epoch(train)  [2][450/491]  lr: 1.0000e-02  eta: 0:53:39  time: 0.5961  data_time: 0.0334  memory: 12672  loss: 2.1060  loss_cls: 0.5235  loss_bbox: 0.6312  loss_dfl: 0.1770  backbone_loss_mask: 0.5904  clip_transfer_loss: 0.1839
2025/07/04 17:30:37 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 17:30:37 - mmengine - INFO - Saving checkpoint at 2 epochs
2025/07/04 17:30:48 - mmengine - INFO - Epoch(val)  [2][ 50/211]    eta: 0:00:31  time: 0.1937  data_time: 0.0377  memory: 12672  
2025/07/04 17:30:58 - mmengine - INFO - Epoch(val)  [2][100/211]    eta: 0:00:21  time: 0.1910  data_time: 0.0369  memory: 1782  
2025/07/04 17:31:08 - mmengine - INFO - Epoch(val)  [2][150/211]    eta: 0:00:11  time: 0.1947  data_time: 0.0397  memory: 1782  
2025/07/04 17:31:18 - mmengine - INFO - Epoch(val)  [2][200/211]    eta: 0:00:02  time: 0.1972  data_time: 0.0422  memory: 1782  
2025/07/04 17:31:22 - mmengine - INFO - Evaluating bbox...
2025/07/04 17:31:32 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.244 | 0.659  | 0.11   | 0.242 | 0.375 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 17:31:32 - mmengine - INFO - bbox_mAP_copypaste: 0.244 0.659 0.110 0.242 0.375 -1.000
2025/07/04 17:31:32 - mmengine - INFO - Epoch(val) [2][211/211]    coco/UAV_precision: 0.2440  coco/bbox_mAP: 0.2440  coco/bbox_mAP_50: 0.6590  coco/bbox_mAP_75: 0.1100  coco/bbox_mAP_s: 0.2420  coco/bbox_mAP_m: 0.3750  coco/bbox_mAP_l: -1.0000  data_time: 0.0385  time: 0.1925
2025/07/04 17:31:43 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 17:32:02 - mmengine - INFO - Epoch(train)  [3][ 50/491]  lr: 1.0000e-02  eta: 0:52:17  time: 0.6070  data_time: 0.0387  memory: 12672  loss: 2.0681  loss_cls: 0.4764  loss_bbox: 0.6422  loss_dfl: 0.1789  backbone_loss_mask: 0.5796  clip_transfer_loss: 0.1909
2025/07/04 17:32:32 - mmengine - INFO - Epoch(train)  [3][100/491]  lr: 1.0000e-02  eta: 0:51:35  time: 0.6011  data_time: 0.0369  memory: 12672  loss: 2.0051  loss_cls: 0.4582  loss_bbox: 0.6221  loss_dfl: 0.1776  backbone_loss_mask: 0.5744  clip_transfer_loss: 0.1728
2025/07/04 17:33:02 - mmengine - INFO - Epoch(train)  [3][150/491]  lr: 1.0000e-02  eta: 0:50:53  time: 0.5952  data_time: 0.0350  memory: 12672  loss: 2.0793  loss_cls: 0.5131  loss_bbox: 0.6567  loss_dfl: 0.1820  backbone_loss_mask: 0.5696  clip_transfer_loss: 0.1578
2025/07/04 17:33:32 - mmengine - INFO - Epoch(train)  [3][200/491]  lr: 1.0000e-02  eta: 0:50:12  time: 0.5979  data_time: 0.0370  memory: 12672  loss: 1.9529  loss_cls: 0.4298  loss_bbox: 0.6430  loss_dfl: 0.1767  backbone_loss_mask: 0.5654  clip_transfer_loss: 0.1380
2025/07/04 17:34:02 - mmengine - INFO - Epoch(train)  [3][250/491]  lr: 1.0000e-02  eta: 0:49:32  time: 0.5979  data_time: 0.0363  memory: 12672  loss: 1.9551  loss_cls: 0.4565  loss_bbox: 0.6405  loss_dfl: 0.1770  backbone_loss_mask: 0.5613  clip_transfer_loss: 0.1199
2025/07/04 17:34:32 - mmengine - INFO - Epoch(train)  [3][300/491]  lr: 1.0000e-02  eta: 0:48:53  time: 0.5992  data_time: 0.0318  memory: 12672  loss: 1.9251  loss_cls: 0.4695  loss_bbox: 0.6123  loss_dfl: 0.1761  backbone_loss_mask: 0.5572  clip_transfer_loss: 0.1100
2025/07/04 17:35:01 - mmengine - INFO - Epoch(train)  [3][350/491]  lr: 1.0000e-02  eta: 0:48:14  time: 0.5947  data_time: 0.0310  memory: 12672  loss: 1.8547  loss_cls: 0.3983  loss_bbox: 0.6268  loss_dfl: 0.1734  backbone_loss_mask: 0.5541  clip_transfer_loss: 0.1020
2025/07/04 17:35:31 - mmengine - INFO - Epoch(train)  [3][400/491]  lr: 1.0000e-02  eta: 0:47:37  time: 0.6002  data_time: 0.0373  memory: 12672  loss: 1.7953  loss_cls: 0.3726  loss_bbox: 0.6028  loss_dfl: 0.1728  backbone_loss_mask: 0.5501  clip_transfer_loss: 0.0970
2025/07/04 17:36:01 - mmengine - INFO - Epoch(train)  [3][450/491]  lr: 1.0000e-02  eta: 0:46:59  time: 0.5951  data_time: 0.0312  memory: 12672  loss: 1.8691  loss_cls: 0.4220  loss_bbox: 0.6205  loss_dfl: 0.1734  backbone_loss_mask: 0.5471  clip_transfer_loss: 0.1061
2025/07/04 17:38:09 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 17:38:09 - mmengine - INFO - Saving checkpoint at 3 epochs
2025/07/04 17:38:30 - mmengine - INFO - Epoch(val)  [3][ 50/211]    eta: 0:00:35  time: 0.2203  data_time: 0.0556  memory: 12672  
2025/07/04 17:38:40 - mmengine - INFO - Epoch(val)  [3][100/211]    eta: 0:00:23  time: 0.1942  data_time: 0.0386  memory: 1782  
2025/07/04 17:38:50 - mmengine - INFO - Epoch(val)  [3][150/211]    eta: 0:00:12  time: 0.1949  data_time: 0.0398  memory: 1782  
2025/07/04 17:38:59 - mmengine - INFO - Epoch(val)  [3][200/211]    eta: 0:00:02  time: 0.1955  data_time: 0.0400  memory: 1782  
2025/07/04 17:39:03 - mmengine - INFO - Evaluating bbox...
2025/07/04 17:39:09 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.234 | 0.635  | 0.116  | 0.225 | 0.574 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 17:39:09 - mmengine - INFO - bbox_mAP_copypaste: 0.234 0.635 0.116 0.225 0.574 -1.000
2025/07/04 17:39:09 - mmengine - INFO - Epoch(val) [3][211/211]    coco/UAV_precision: 0.2340  coco/bbox_mAP: 0.2340  coco/bbox_mAP_50: 0.6350  coco/bbox_mAP_75: 0.1160  coco/bbox_mAP_s: 0.2250  coco/bbox_mAP_m: 0.5740  coco/bbox_mAP_l: -1.0000  data_time: 0.0426  time: 0.1991
2025/07/04 17:39:40 - mmengine - INFO - Epoch(train)  [4][ 50/491]  lr: 1.0000e-02  eta: 0:50:52  time: 0.6182  data_time: 0.0390  memory: 12672  loss: 1.7650  loss_cls: 0.3440  loss_bbox: 0.6152  loss_dfl: 0.1741  backbone_loss_mask: 0.5415  clip_transfer_loss: 0.0902
2025/07/04 17:40:11 - mmengine - INFO - Epoch(train)  [4][100/491]  lr: 1.0000e-02  eta: 0:50:04  time: 0.6055  data_time: 0.0372  memory: 12672  loss: 1.8064  loss_cls: 0.3868  loss_bbox: 0.6134  loss_dfl: 0.1719  backbone_loss_mask: 0.5386  clip_transfer_loss: 0.0956
2025/07/04 17:40:41 - mmengine - INFO - Epoch(train)  [4][150/491]  lr: 1.0000e-02  eta: 0:49:18  time: 0.6045  data_time: 0.0391  memory: 12672  loss: 1.7126  loss_cls: 0.3346  loss_bbox: 0.5723  loss_dfl: 0.1708  backbone_loss_mask: 0.5354  clip_transfer_loss: 0.0996
2025/07/04 17:41:11 - mmengine - INFO - Epoch(train)  [4][200/491]  lr: 1.0000e-02  eta: 0:48:31  time: 0.5993  data_time: 0.0379  memory: 12672  loss: 1.7278  loss_cls: 0.3692  loss_bbox: 0.5643  loss_dfl: 0.1684  backbone_loss_mask: 0.5325  clip_transfer_loss: 0.0934
2025/07/04 17:41:41 - mmengine - INFO - Epoch(train)  [4][250/491]  lr: 1.0000e-02  eta: 0:47:46  time: 0.6021  data_time: 0.0383  memory: 12672  loss: 1.7376  loss_cls: 0.3302  loss_bbox: 0.6124  loss_dfl: 0.1704  backbone_loss_mask: 0.5299  clip_transfer_loss: 0.0947
2025/07/04 17:42:11 - mmengine - INFO - Epoch(train)  [4][300/491]  lr: 1.0000e-02  eta: 0:47:01  time: 0.5968  data_time: 0.0356  memory: 12672  loss: 1.7546  loss_cls: 0.3682  loss_bbox: 0.6049  loss_dfl: 0.1700  backbone_loss_mask: 0.5265  clip_transfer_loss: 0.0850
2025/07/04 17:48:50 - mmengine - INFO - Epoch(train)  [4][350/491]  lr: 1.0000e-02  eta: 1:00:02  time: 7.9929  data_time: 0.1219  memory: 12672  loss: 1.7877  loss_cls: 0.3804  loss_bbox: 0.6182  loss_dfl: 0.1731  backbone_loss_mask: 0.5249  clip_transfer_loss: 0.0911
2025/07/04 18:00:06 - mmengine - INFO - Epoch(train)  [4][400/491]  lr: 1.0000e-02  eta: 1:21:53  time: 13.5120  data_time: 0.1957  memory: 12672  loss: 1.7818  loss_cls: 0.3909  loss_bbox: 0.6066  loss_dfl: 0.1716  backbone_loss_mask: 0.5211  clip_transfer_loss: 0.0916
2025/07/04 18:11:36 - mmengine - INFO - Epoch(train)  [4][450/491]  lr: 1.0000e-02  eta: 1:42:30  time: 13.8079  data_time: 0.1950  memory: 12672  loss: 1.7234  loss_cls: 0.3585  loss_bbox: 0.5929  loss_dfl: 0.1712  backbone_loss_mask: 0.5172  clip_transfer_loss: 0.0836
2025/07/04 18:20:12 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 18:20:12 - mmengine - INFO - Saving checkpoint at 4 epochs
2025/07/04 18:21:16 - mmengine - INFO - Epoch(val)  [4][ 50/211]    eta: 0:02:48  time: 1.0445  data_time: 0.2096  memory: 12672  
2025/07/04 18:22:07 - mmengine - INFO - Epoch(val)  [4][100/211]    eta: 0:01:55  time: 1.0320  data_time: 0.1928  memory: 1782  
2025/07/04 18:22:59 - mmengine - INFO - Epoch(val)  [4][150/211]    eta: 0:01:03  time: 1.0340  data_time: 0.1938  memory: 1782  
2025/07/04 18:23:52 - mmengine - INFO - Epoch(val)  [4][200/211]    eta: 0:00:11  time: 1.0604  data_time: 0.1985  memory: 1782  
2025/07/04 18:24:14 - mmengine - INFO - Evaluating bbox...
2025/07/04 18:24:53 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.299 | 0.733  | 0.183  | 0.293 | 0.53  | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 18:24:53 - mmengine - INFO - bbox_mAP_copypaste: 0.299 0.733 0.183 0.293 0.530 -1.000
2025/07/04 18:24:53 - mmengine - INFO - Epoch(val) [4][211/211]    coco/UAV_precision: 0.2990  coco/bbox_mAP: 0.2990  coco/bbox_mAP_50: 0.7330  coco/bbox_mAP_75: 0.1830  coco/bbox_mAP_s: 0.2930  coco/bbox_mAP_m: 0.5300  coco/bbox_mAP_l: -1.0000  data_time: 0.1969  time: 1.0368
2025/07/04 18:31:43 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 18:34:27 - mmengine - INFO - Epoch(train)  [5][ 50/491]  lr: 1.0000e-02  eta: 2:10:35  time: 11.4740  data_time: 0.2026  memory: 12672  loss: 1.6703  loss_cls: 0.3164  loss_bbox: 0.5873  loss_dfl: 0.1700  backbone_loss_mask: 0.5127  clip_transfer_loss: 0.0840
2025/07/04 18:43:49 - mmengine - INFO - Epoch(train)  [5][100/491]  lr: 1.0000e-02  eta: 2:23:10  time: 11.2438  data_time: 0.1834  memory: 12672  loss: 1.6608  loss_cls: 0.3158  loss_bbox: 0.5803  loss_dfl: 0.1673  backbone_loss_mask: 0.5100  clip_transfer_loss: 0.0874
2025/07/04 18:53:14 - mmengine - INFO - Epoch(train)  [5][150/491]  lr: 1.0000e-02  eta: 2:34:46  time: 11.2959  data_time: 0.2097  memory: 12672  loss: 1.7120  loss_cls: 0.3371  loss_bbox: 0.6074  loss_dfl: 0.1717  backbone_loss_mask: 0.5087  clip_transfer_loss: 0.0871
2025/07/04 19:02:23 - mmengine - INFO - Epoch(train)  [5][200/491]  lr: 1.0000e-02  eta: 2:44:57  time: 10.9760  data_time: 0.1936  memory: 12672  loss: 1.7422  loss_cls: 0.3667  loss_bbox: 0.6058  loss_dfl: 0.1724  backbone_loss_mask: 0.5039  clip_transfer_loss: 0.0934
2025/07/04 19:11:43 - mmengine - INFO - Epoch(train)  [5][250/491]  lr: 1.0000e-02  eta: 2:54:34  time: 11.2000  data_time: 0.1900  memory: 12672  loss: 1.6406  loss_cls: 0.3063  loss_bbox: 0.5861  loss_dfl: 0.1695  backbone_loss_mask: 0.4979  clip_transfer_loss: 0.0808
2025/07/04 19:21:04 - mmengine - INFO - Epoch(train)  [5][300/491]  lr: 1.0000e-02  eta: 3:03:22  time: 11.2160  data_time: 0.1936  memory: 12672  loss: 1.6418  loss_cls: 0.3227  loss_bbox: 0.5804  loss_dfl: 0.1677  backbone_loss_mask: 0.4941  clip_transfer_loss: 0.0769
2025/07/04 19:29:21 - mmengine - INFO - Epoch(train)  [5][350/491]  lr: 1.0000e-02  eta: 3:09:45  time: 9.9446  data_time: 0.1921  memory: 12672  loss: 1.6743  loss_cls: 0.3383  loss_bbox: 0.5950  loss_dfl: 0.1679  backbone_loss_mask: 0.4928  clip_transfer_loss: 0.0801
2025/07/04 19:38:31 - mmengine - INFO - Epoch(train)  [5][400/491]  lr: 1.0000e-02  eta: 3:16:49  time: 10.9919  data_time: 0.1887  memory: 12672  loss: 1.6723  loss_cls: 0.3575  loss_bbox: 0.5776  loss_dfl: 0.1686  backbone_loss_mask: 0.4909  clip_transfer_loss: 0.0777
2025/07/04 19:47:47 - mmengine - INFO - Epoch(train)  [5][450/491]  lr: 1.0000e-02  eta: 3:23:21  time: 11.1180  data_time: 0.1958  memory: 12672  loss: 1.6102  loss_cls: 0.3173  loss_bbox: 0.5598  loss_dfl: 0.1660  backbone_loss_mask: 0.4867  clip_transfer_loss: 0.0804
2025/07/04 19:55:24 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 19:55:24 - mmengine - INFO - Saving checkpoint at 5 epochs
2025/07/04 19:56:27 - mmengine - INFO - Epoch(val)  [5][ 50/211]    eta: 0:02:47  time: 1.0402  data_time: 0.2109  memory: 12672  
2025/07/04 19:57:19 - mmengine - INFO - Epoch(val)  [5][100/211]    eta: 0:01:55  time: 1.0378  data_time: 0.2067  memory: 1782  
2025/07/04 19:58:09 - mmengine - INFO - Epoch(val)  [5][150/211]    eta: 0:01:02  time: 1.0108  data_time: 0.1869  memory: 1782  
2025/07/04 19:59:00 - mmengine - INFO - Epoch(val)  [5][200/211]    eta: 0:00:11  time: 1.0159  data_time: 0.1982  memory: 1782  
2025/07/04 19:59:17 - mmengine - INFO - Evaluating bbox...
2025/07/04 19:59:44 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.296 | 0.759  | 0.145  | 0.291 | 0.538 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 19:59:44 - mmengine - INFO - bbox_mAP_copypaste: 0.296 0.759 0.145 0.291 0.538 -1.000
2025/07/04 19:59:45 - mmengine - INFO - Epoch(val) [5][211/211]    coco/UAV_precision: 0.2960  coco/bbox_mAP: 0.2960  coco/bbox_mAP_50: 0.7590  coco/bbox_mAP_75: 0.1450  coco/bbox_mAP_s: 0.2910  coco/bbox_mAP_m: 0.5380  coco/bbox_mAP_l: -1.0000  data_time: 0.1984  time: 1.0190
2025/07/04 20:09:08 - mmengine - INFO - Epoch(train)  [6][ 50/491]  lr: 1.0000e-02  eta: 3:33:51  time: 11.2720  data_time: 0.2065  memory: 12672  loss: 1.7434  loss_cls: 0.4175  loss_bbox: 0.5926  loss_dfl: 0.1722  backbone_loss_mask: 0.4795  clip_transfer_loss: 0.0816
2025/07/04 20:19:05 - mmengine - INFO - Epoch(train)  [6][100/491]  lr: 1.0000e-02  eta: 3:39:33  time: 11.9221  data_time: 0.1944  memory: 12672  loss: 1.6189  loss_cls: 0.3208  loss_bbox: 0.5746  loss_dfl: 0.1696  backbone_loss_mask: 0.4738  clip_transfer_loss: 0.0803
2025/07/04 20:29:32 - mmengine - INFO - Epoch(train)  [6][150/491]  lr: 1.0000e-02  eta: 3:45:18  time: 12.5497  data_time: 0.2000  memory: 12672  loss: 1.6613  loss_cls: 0.3516  loss_bbox: 0.5861  loss_dfl: 0.1678  backbone_loss_mask: 0.4718  clip_transfer_loss: 0.0841
2025/07/04 20:39:38 - mmengine - INFO - Epoch(train)  [6][200/491]  lr: 1.0000e-02  eta: 3:50:01  time: 12.1180  data_time: 0.2003  memory: 12672  loss: 1.6687  loss_cls: 0.3737  loss_bbox: 0.5818  loss_dfl: 0.1639  backbone_loss_mask: 0.4666  clip_transfer_loss: 0.0828
2025/07/04 20:50:28 - mmengine - INFO - Epoch(train)  [6][250/491]  lr: 1.0000e-02  eta: 3:55:03  time: 13.0099  data_time: 0.2022  memory: 12672  loss: 1.6111  loss_cls: 0.3362  loss_bbox: 0.5646  loss_dfl: 0.1684  backbone_loss_mask: 0.4586  clip_transfer_loss: 0.0833
2025/07/04 21:01:21 - mmengine - INFO - Epoch(train)  [6][300/491]  lr: 1.0000e-02  eta: 3:59:33  time: 13.0600  data_time: 0.1981  memory: 12672  loss: 1.6088  loss_cls: 0.3360  loss_bbox: 0.5723  loss_dfl: 0.1682  backbone_loss_mask: 0.4537  clip_transfer_loss: 0.0787
2025/07/04 21:12:29 - mmengine - INFO - Epoch(train)  [6][350/491]  lr: 1.0000e-02  eta: 4:03:47  time: 13.3560  data_time: 0.2137  memory: 12672  loss: 1.5599  loss_cls: 0.3067  loss_bbox: 0.5757  loss_dfl: 0.1677  backbone_loss_mask: 0.4438  clip_transfer_loss: 0.0660
2025/07/04 21:23:11 - mmengine - INFO - Epoch(train)  [6][400/491]  lr: 1.0000e-02  eta: 4:07:00  time: 12.8339  data_time: 0.1850  memory: 12672  loss: 1.5561  loss_cls: 0.3173  loss_bbox: 0.5613  loss_dfl: 0.1659  backbone_loss_mask: 0.4405  clip_transfer_loss: 0.0711
2025/07/04 21:34:01 - mmengine - INFO - Epoch(train)  [6][450/491]  lr: 1.0000e-02  eta: 4:09:54  time: 12.9980  data_time: 0.1893  memory: 12672  loss: 1.5636  loss_cls: 0.3171  loss_bbox: 0.5722  loss_dfl: 0.1667  backbone_loss_mask: 0.4331  clip_transfer_loss: 0.0745
2025/07/04 21:41:34 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 21:41:34 - mmengine - INFO - Saving checkpoint at 6 epochs
2025/07/04 21:41:46 - mmengine - INFO - Epoch(val)  [6][ 50/211]    eta: 0:00:32  time: 0.2013  data_time: 0.0418  memory: 12672  
2025/07/04 21:41:56 - mmengine - INFO - Epoch(val)  [6][100/211]    eta: 0:00:21  time: 0.1914  data_time: 0.0373  memory: 1782  
2025/07/04 21:42:05 - mmengine - INFO - Epoch(val)  [6][150/211]    eta: 0:00:11  time: 0.1936  data_time: 0.0383  memory: 1782  
2025/07/04 21:42:15 - mmengine - INFO - Epoch(val)  [6][200/211]    eta: 0:00:02  time: 0.1924  data_time: 0.0373  memory: 1782  
2025/07/04 21:42:18 - mmengine - INFO - Evaluating bbox...
2025/07/04 21:42:22 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.312 | 0.768  | 0.188  | 0.308 | 0.565 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 21:42:22 - mmengine - INFO - bbox_mAP_copypaste: 0.312 0.768 0.188 0.308 0.565 -1.000
2025/07/04 21:42:22 - mmengine - INFO - Epoch(val) [6][211/211]    coco/UAV_precision: 0.3120  coco/bbox_mAP: 0.3120  coco/bbox_mAP_50: 0.7680  coco/bbox_mAP_75: 0.1880  coco/bbox_mAP_s: 0.3080  coco/bbox_mAP_m: 0.5650  coco/bbox_mAP_l: -1.0000  data_time: 0.0380  time: 0.1950
2025/07/04 21:42:53 - mmengine - INFO - Epoch(train)  [7][ 50/491]  lr: 1.0000e-02  eta: 4:02:43  time: 0.6098  data_time: 0.0416  memory: 12672  loss: 1.4516  loss_cls: 0.2407  loss_bbox: 0.5472  loss_dfl: 0.1651  backbone_loss_mask: 0.4188  clip_transfer_loss: 0.0798
2025/07/04 21:42:55 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 21:43:24 - mmengine - INFO - Epoch(train)  [7][100/491]  lr: 1.0000e-02  eta: 3:55:05  time: 0.6104  data_time: 0.0361  memory: 12672  loss: 1.4688  loss_cls: 0.2620  loss_bbox: 0.5486  loss_dfl: 0.1672  backbone_loss_mask: 0.4134  clip_transfer_loss: 0.0775
2025/07/04 21:43:54 - mmengine - INFO - Epoch(train)  [7][150/491]  lr: 1.0000e-02  eta: 3:47:41  time: 0.6020  data_time: 0.0382  memory: 12672  loss: 1.4465  loss_cls: 0.2554  loss_bbox: 0.5431  loss_dfl: 0.1617  backbone_loss_mask: 0.4040  clip_transfer_loss: 0.0823
2025/07/04 21:44:24 - mmengine - INFO - Epoch(train)  [7][200/491]  lr: 1.0000e-02  eta: 3:40:30  time: 0.6062  data_time: 0.0382  memory: 12672  loss: 1.4313  loss_cls: 0.2556  loss_bbox: 0.5357  loss_dfl: 0.1629  backbone_loss_mask: 0.3990  clip_transfer_loss: 0.0781
2025/07/04 21:44:54 - mmengine - INFO - Epoch(train)  [7][250/491]  lr: 1.0000e-02  eta: 3:33:31  time: 0.6100  data_time: 0.0367  memory: 12672  loss: 1.4741  loss_cls: 0.2755  loss_bbox: 0.5679  loss_dfl: 0.1692  backbone_loss_mask: 0.3922  clip_transfer_loss: 0.0694
2025/07/04 21:45:24 - mmengine - INFO - Epoch(train)  [7][300/491]  lr: 1.0000e-02  eta: 3:26:44  time: 0.5994  data_time: 0.0358  memory: 12672  loss: 1.4511  loss_cls: 0.2662  loss_bbox: 0.5551  loss_dfl: 0.1638  backbone_loss_mask: 0.3892  clip_transfer_loss: 0.0767
2025/07/04 21:45:55 - mmengine - INFO - Epoch(train)  [7][350/491]  lr: 1.0000e-02  eta: 3:20:09  time: 0.6024  data_time: 0.0347  memory: 12672  loss: 1.4497  loss_cls: 0.2635  loss_bbox: 0.5585  loss_dfl: 0.1683  backbone_loss_mask: 0.3834  clip_transfer_loss: 0.0759
2025/07/04 21:46:25 - mmengine - INFO - Epoch(train)  [7][400/491]  lr: 1.0000e-02  eta: 3:13:45  time: 0.6115  data_time: 0.0394  memory: 12672  loss: 1.4197  loss_cls: 0.2540  loss_bbox: 0.5499  loss_dfl: 0.1634  backbone_loss_mask: 0.3818  clip_transfer_loss: 0.0705
2025/07/04 21:46:55 - mmengine - INFO - Epoch(train)  [7][450/491]  lr: 1.0000e-02  eta: 3:07:31  time: 0.6045  data_time: 0.0376  memory: 12672  loss: 1.4212  loss_cls: 0.2693  loss_bbox: 0.5466  loss_dfl: 0.1649  backbone_loss_mask: 0.3699  clip_transfer_loss: 0.0706
2025/07/04 21:47:20 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 21:47:20 - mmengine - INFO - Saving checkpoint at 7 epochs
2025/07/04 21:47:32 - mmengine - INFO - Epoch(val)  [7][ 50/211]    eta: 0:00:32  time: 0.2000  data_time: 0.0417  memory: 12672  
2025/07/04 21:47:42 - mmengine - INFO - Epoch(val)  [7][100/211]    eta: 0:00:21  time: 0.1946  data_time: 0.0380  memory: 1782  
2025/07/04 21:47:51 - mmengine - INFO - Epoch(val)  [7][150/211]    eta: 0:00:11  time: 0.1948  data_time: 0.0389  memory: 1782  
2025/07/04 21:48:01 - mmengine - INFO - Epoch(val)  [7][200/211]    eta: 0:00:02  time: 0.1951  data_time: 0.0383  memory: 1782  
2025/07/04 21:48:04 - mmengine - INFO - Evaluating bbox...
2025/07/04 21:48:08 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.283 | 0.724  | 0.149  | 0.277 | 0.538 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 21:48:08 - mmengine - INFO - bbox_mAP_copypaste: 0.283 0.724 0.149 0.277 0.538 -1.000
2025/07/04 21:48:08 - mmengine - INFO - Epoch(val) [7][211/211]    coco/UAV_precision: 0.2830  coco/bbox_mAP: 0.2830  coco/bbox_mAP_50: 0.7240  coco/bbox_mAP_75: 0.1490  coco/bbox_mAP_s: 0.2770  coco/bbox_mAP_m: 0.5380  coco/bbox_mAP_l: -1.0000  data_time: 0.0396  time: 0.1953
2025/07/04 21:48:39 - mmengine - INFO - Epoch(train)  [8][ 50/491]  lr: 1.0000e-02  eta: 2:56:36  time: 0.6268  data_time: 0.0423  memory: 12672  loss: 1.4832  loss_cls: 0.3279  loss_bbox: 0.5483  loss_dfl: 0.1649  backbone_loss_mask: 0.3672  clip_transfer_loss: 0.0749
2025/07/04 21:49:09 - mmengine - INFO - Epoch(train)  [8][100/491]  lr: 1.0000e-02  eta: 2:50:49  time: 0.6005  data_time: 0.0382  memory: 12672  loss: 1.4701  loss_cls: 0.3349  loss_bbox: 0.5367  loss_dfl: 0.1606  backbone_loss_mask: 0.3649  clip_transfer_loss: 0.0729
2025/07/04 21:49:40 - mmengine - INFO - Epoch(train)  [8][150/491]  lr: 1.0000e-02  eta: 2:45:11  time: 0.6079  data_time: 0.0385  memory: 12672  loss: 1.4030  loss_cls: 0.2888  loss_bbox: 0.5227  loss_dfl: 0.1605  backbone_loss_mask: 0.3574  clip_transfer_loss: 0.0736
2025/07/04 21:50:10 - mmengine - INFO - Epoch(train)  [8][200/491]  lr: 1.0000e-02  eta: 2:39:42  time: 0.6100  data_time: 0.0397  memory: 12672  loss: 1.4479  loss_cls: 0.3093  loss_bbox: 0.5495  loss_dfl: 0.1649  backbone_loss_mask: 0.3512  clip_transfer_loss: 0.0730
2025/07/04 21:50:41 - mmengine - INFO - Epoch(train)  [8][250/491]  lr: 1.0000e-02  eta: 2:34:21  time: 0.6100  data_time: 0.0385  memory: 12672  loss: 1.3726  loss_cls: 0.2589  loss_bbox: 0.5311  loss_dfl: 0.1615  backbone_loss_mask: 0.3477  clip_transfer_loss: 0.0734
2025/07/04 21:51:11 - mmengine - INFO - Epoch(train)  [8][300/491]  lr: 1.0000e-02  eta: 2:29:07  time: 0.6050  data_time: 0.0377  memory: 12672  loss: 1.3860  loss_cls: 0.2471  loss_bbox: 0.5582  loss_dfl: 0.1646  backbone_loss_mask: 0.3427  clip_transfer_loss: 0.0734
2025/07/04 21:51:41 - mmengine - INFO - Epoch(train)  [8][350/491]  lr: 1.0000e-02  eta: 2:24:01  time: 0.6129  data_time: 0.0368  memory: 12672  loss: 1.3658  loss_cls: 0.2400  loss_bbox: 0.5521  loss_dfl: 0.1641  backbone_loss_mask: 0.3379  clip_transfer_loss: 0.0717
2025/07/04 21:52:12 - mmengine - INFO - Epoch(train)  [8][400/491]  lr: 1.0000e-02  eta: 2:19:02  time: 0.6061  data_time: 0.0373  memory: 12672  loss: 1.3008  loss_cls: 0.2127  loss_bbox: 0.5219  loss_dfl: 0.1636  backbone_loss_mask: 0.3322  clip_transfer_loss: 0.0704
2025/07/04 21:52:42 - mmengine - INFO - Epoch(train)  [8][450/491]  lr: 1.0000e-02  eta: 2:14:10  time: 0.6076  data_time: 0.0391  memory: 12672  loss: 1.3252  loss_cls: 0.2454  loss_bbox: 0.5245  loss_dfl: 0.1608  backbone_loss_mask: 0.3244  clip_transfer_loss: 0.0700
2025/07/04 21:53:06 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 21:53:06 - mmengine - INFO - Saving checkpoint at 8 epochs
2025/07/04 21:53:19 - mmengine - INFO - Epoch(val)  [8][ 50/211]    eta: 0:00:31  time: 0.1987  data_time: 0.0430  memory: 12672  
2025/07/04 21:53:28 - mmengine - INFO - Epoch(val)  [8][100/211]    eta: 0:00:21  time: 0.1909  data_time: 0.0373  memory: 1782  
2025/07/04 21:53:38 - mmengine - INFO - Epoch(val)  [8][150/211]    eta: 0:00:11  time: 0.1911  data_time: 0.0377  memory: 1782  
2025/07/04 21:53:47 - mmengine - INFO - Epoch(val)  [8][200/211]    eta: 0:00:02  time: 0.1922  data_time: 0.0366  memory: 1782  
2025/07/04 21:53:50 - mmengine - INFO - Evaluating bbox...
2025/07/04 21:53:55 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.308 | 0.778  | 0.182  | 0.3   | 0.624 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 21:53:55 - mmengine - INFO - bbox_mAP_copypaste: 0.308 0.778 0.182 0.300 0.624 -1.000
2025/07/04 21:53:55 - mmengine - INFO - Epoch(val) [8][211/211]    coco/UAV_precision: 0.3080  coco/bbox_mAP: 0.3080  coco/bbox_mAP_50: 0.7780  coco/bbox_mAP_75: 0.1820  coco/bbox_mAP_s: 0.3000  coco/bbox_mAP_m: 0.6240  coco/bbox_mAP_l: -1.0000  data_time: 0.0382  time: 0.1917
2025/07/04 21:54:25 - mmengine - INFO - Epoch(train)  [9][ 50/491]  lr: 1.0000e-03  eta: 2:05:35  time: 0.6043  data_time: 0.0428  memory: 12672  loss: 1.2445  loss_cls: 0.2064  loss_bbox: 0.4978  loss_dfl: 0.1564  backbone_loss_mask: 0.3184  clip_transfer_loss: 0.0655
2025/07/04 21:54:38 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 21:54:55 - mmengine - INFO - Epoch(train)  [9][100/491]  lr: 1.0000e-03  eta: 2:01:01  time: 0.5999  data_time: 0.0361  memory: 12672  loss: 1.2121  loss_cls: 0.1994  loss_bbox: 0.4714  loss_dfl: 0.1575  backbone_loss_mask: 0.3181  clip_transfer_loss: 0.0658
2025/07/04 21:55:25 - mmengine - INFO - Epoch(train)  [9][150/491]  lr: 1.0000e-03  eta: 1:56:33  time: 0.6057  data_time: 0.0362  memory: 12672  loss: 1.2237  loss_cls: 0.1901  loss_bbox: 0.4816  loss_dfl: 0.1555  backbone_loss_mask: 0.3248  clip_transfer_loss: 0.0717
2025/07/04 21:55:56 - mmengine - INFO - Epoch(train)  [9][200/491]  lr: 1.0000e-03  eta: 1:52:11  time: 0.6083  data_time: 0.0350  memory: 12672  loss: 1.1904  loss_cls: 0.1859  loss_bbox: 0.4682  loss_dfl: 0.1541  backbone_loss_mask: 0.3080  clip_transfer_loss: 0.0742
2025/07/04 21:56:25 - mmengine - INFO - Epoch(train)  [9][250/491]  lr: 1.0000e-03  eta: 1:47:54  time: 0.5939  data_time: 0.0359  memory: 12672  loss: 1.1710  loss_cls: 0.1838  loss_bbox: 0.4503  loss_dfl: 0.1520  backbone_loss_mask: 0.3145  clip_transfer_loss: 0.0705
2025/07/04 21:56:55 - mmengine - INFO - Epoch(train)  [9][300/491]  lr: 1.0000e-03  eta: 1:43:43  time: 0.5975  data_time: 0.0374  memory: 12672  loss: 1.1776  loss_cls: 0.1793  loss_bbox: 0.4537  loss_dfl: 0.1537  backbone_loss_mask: 0.3216  clip_transfer_loss: 0.0693
2025/07/04 21:57:25 - mmengine - INFO - Epoch(train)  [9][350/491]  lr: 1.0000e-03  eta: 1:39:36  time: 0.6063  data_time: 0.0366  memory: 12672  loss: 1.1712  loss_cls: 0.1799  loss_bbox: 0.4550  loss_dfl: 0.1534  backbone_loss_mask: 0.3136  clip_transfer_loss: 0.0693
2025/07/04 21:57:56 - mmengine - INFO - Epoch(train)  [9][400/491]  lr: 1.0000e-03  eta: 1:35:35  time: 0.6006  data_time: 0.0381  memory: 12672  loss: 1.1980  loss_cls: 0.1847  loss_bbox: 0.4754  loss_dfl: 0.1540  backbone_loss_mask: 0.3163  clip_transfer_loss: 0.0676
2025/07/04 21:58:25 - mmengine - INFO - Epoch(train)  [9][450/491]  lr: 1.0000e-03  eta: 1:31:39  time: 0.5973  data_time: 0.0354  memory: 12672  loss: 1.1888  loss_cls: 0.1850  loss_bbox: 0.4691  loss_dfl: 0.1532  backbone_loss_mask: 0.3141  clip_transfer_loss: 0.0674
2025/07/04 21:58:49 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 21:58:49 - mmengine - INFO - Saving checkpoint at 9 epochs
2025/07/04 21:59:01 - mmengine - INFO - Epoch(val)  [9][ 50/211]    eta: 0:00:31  time: 0.1967  data_time: 0.0387  memory: 12672  
2025/07/04 21:59:11 - mmengine - INFO - Epoch(val)  [9][100/211]    eta: 0:00:21  time: 0.1922  data_time: 0.0370  memory: 1782  
2025/07/04 21:59:21 - mmengine - INFO - Epoch(val)  [9][150/211]    eta: 0:00:11  time: 0.1957  data_time: 0.0398  memory: 1782  
2025/07/04 21:59:30 - mmengine - INFO - Epoch(val)  [9][200/211]    eta: 0:00:02  time: 0.1928  data_time: 0.0378  memory: 1782  
2025/07/04 21:59:33 - mmengine - INFO - Evaluating bbox...
2025/07/04 21:59:36 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.393 | 0.857  | 0.284  | 0.389 | 0.627 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 21:59:36 - mmengine - INFO - bbox_mAP_copypaste: 0.393 0.857 0.284 0.389 0.627 -1.000
2025/07/04 21:59:36 - mmengine - INFO - Epoch(val) [9][211/211]    coco/UAV_precision: 0.3930  coco/bbox_mAP: 0.3930  coco/bbox_mAP_50: 0.8570  coco/bbox_mAP_75: 0.2840  coco/bbox_mAP_s: 0.3890  coco/bbox_mAP_m: 0.6270  coco/bbox_mAP_l: -1.0000  data_time: 0.0380  time: 0.1927
2025/07/04 22:00:07 - mmengine - INFO - Epoch(train) [10][ 50/491]  lr: 1.0000e-03  eta: 1:24:40  time: 0.6124  data_time: 0.0396  memory: 12672  loss: 1.1714  loss_cls: 0.1725  loss_bbox: 0.4540  loss_dfl: 0.1519  backbone_loss_mask: 0.3186  clip_transfer_loss: 0.0743
2025/07/04 22:00:36 - mmengine - INFO - Epoch(train) [10][100/491]  lr: 1.0000e-03  eta: 1:20:57  time: 0.5939  data_time: 0.0337  memory: 12672  loss: 1.1902  loss_cls: 0.1701  loss_bbox: 0.4671  loss_dfl: 0.1557  backbone_loss_mask: 0.3166  clip_transfer_loss: 0.0807
2025/07/04 22:01:07 - mmengine - INFO - Epoch(train) [10][150/491]  lr: 1.0000e-03  eta: 1:17:17  time: 0.6042  data_time: 0.0338  memory: 12672  loss: 1.1506  loss_cls: 0.1789  loss_bbox: 0.4367  loss_dfl: 0.1503  backbone_loss_mask: 0.3100  clip_transfer_loss: 0.0748
2025/07/04 22:01:38 - mmengine - INFO - Epoch(train) [10][200/491]  lr: 1.0000e-03  eta: 1:13:42  time: 0.6239  data_time: 0.0389  memory: 12672  loss: 1.1184  loss_cls: 0.1606  loss_bbox: 0.4311  loss_dfl: 0.1505  backbone_loss_mask: 0.3079  clip_transfer_loss: 0.0683
2025/07/04 22:02:09 - mmengine - INFO - Epoch(train) [10][250/491]  lr: 1.0000e-03  eta: 1:10:11  time: 0.6240  data_time: 0.0402  memory: 12672  loss: 1.1601  loss_cls: 0.1718  loss_bbox: 0.4482  loss_dfl: 0.1515  backbone_loss_mask: 0.3146  clip_transfer_loss: 0.0740
2025/07/04 22:02:40 - mmengine - INFO - Epoch(train) [10][300/491]  lr: 1.0000e-03  eta: 1:06:44  time: 0.6167  data_time: 0.0399  memory: 12672  loss: 1.1179  loss_cls: 0.1557  loss_bbox: 0.4439  loss_dfl: 0.1534  backbone_loss_mask: 0.3008  clip_transfer_loss: 0.0641
2025/07/04 22:03:10 - mmengine - INFO - Epoch(train) [10][350/491]  lr: 1.0000e-03  eta: 1:03:20  time: 0.6048  data_time: 0.0370  memory: 12672  loss: 1.1265  loss_cls: 0.1618  loss_bbox: 0.4422  loss_dfl: 0.1525  backbone_loss_mask: 0.3074  clip_transfer_loss: 0.0626
2025/07/04 22:03:40 - mmengine - INFO - Epoch(train) [10][400/491]  lr: 1.0000e-03  eta: 1:00:00  time: 0.5987  data_time: 0.0363  memory: 12672  loss: 1.1372  loss_cls: 0.1636  loss_bbox: 0.4512  loss_dfl: 0.1509  backbone_loss_mask: 0.3096  clip_transfer_loss: 0.0620
2025/07/04 22:04:10 - mmengine - INFO - Epoch(train) [10][450/491]  lr: 1.0000e-03  eta: 0:56:43  time: 0.6018  data_time: 0.0342  memory: 12672  loss: 1.1447  loss_cls: 0.1700  loss_bbox: 0.4434  loss_dfl: 0.1507  backbone_loss_mask: 0.3096  clip_transfer_loss: 0.0710
2025/07/04 22:04:35 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 22:04:35 - mmengine - INFO - Saving checkpoint at 10 epochs
2025/07/04 22:04:47 - mmengine - INFO - Epoch(val) [10][ 50/211]    eta: 0:00:31  time: 0.1962  data_time: 0.0404  memory: 12672  
2025/07/04 22:04:56 - mmengine - INFO - Epoch(val) [10][100/211]    eta: 0:00:21  time: 0.1937  data_time: 0.0382  memory: 1782  
2025/07/04 22:05:06 - mmengine - INFO - Epoch(val) [10][150/211]    eta: 0:00:11  time: 0.1990  data_time: 0.0433  memory: 1782  
2025/07/04 22:05:16 - mmengine - INFO - Epoch(val) [10][200/211]    eta: 0:00:02  time: 0.1933  data_time: 0.0381  memory: 1782  
2025/07/04 22:05:18 - mmengine - INFO - Evaluating bbox...
2025/07/04 22:05:21 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.401 | 0.868  | 0.302  | 0.397 | 0.631 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 22:05:21 - mmengine - INFO - bbox_mAP_copypaste: 0.401 0.868 0.302 0.397 0.631 -1.000
2025/07/04 22:05:21 - mmengine - INFO - Epoch(val) [10][211/211]    coco/UAV_precision: 0.4010  coco/bbox_mAP: 0.4010  coco/bbox_mAP_50: 0.8680  coco/bbox_mAP_75: 0.3020  coco/bbox_mAP_s: 0.3970  coco/bbox_mAP_m: 0.6310  coco/bbox_mAP_l: -1.0000  data_time: 0.0394  time: 0.1937
2025/07/04 22:05:52 - mmengine - INFO - Epoch(train) [11][ 50/491]  lr: 1.0000e-03  eta: 0:50:54  time: 0.6111  data_time: 0.0401  memory: 12672  loss: 1.0903  loss_cls: 0.1491  loss_bbox: 0.4252  loss_dfl: 0.1478  backbone_loss_mask: 0.2989  clip_transfer_loss: 0.0693
2025/07/04 22:06:16 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 22:06:22 - mmengine - INFO - Epoch(train) [11][100/491]  lr: 1.0000e-03  eta: 0:47:47  time: 0.6013  data_time: 0.0364  memory: 12672  loss: 1.1226  loss_cls: 0.1542  loss_bbox: 0.4363  loss_dfl: 0.1519  backbone_loss_mask: 0.3094  clip_transfer_loss: 0.0707
2025/07/04 22:06:52 - mmengine - INFO - Epoch(train) [11][150/491]  lr: 1.0000e-03  eta: 0:44:42  time: 0.6045  data_time: 0.0375  memory: 12672  loss: 1.1143  loss_cls: 0.1551  loss_bbox: 0.4262  loss_dfl: 0.1517  backbone_loss_mask: 0.3118  clip_transfer_loss: 0.0695
2025/07/04 22:07:22 - mmengine - INFO - Epoch(train) [11][200/491]  lr: 1.0000e-03  eta: 0:41:41  time: 0.6038  data_time: 0.0407  memory: 12672  loss: 1.1286  loss_cls: 0.1683  loss_bbox: 0.4359  loss_dfl: 0.1511  backbone_loss_mask: 0.3039  clip_transfer_loss: 0.0693
2025/07/04 22:07:52 - mmengine - INFO - Epoch(train) [11][250/491]  lr: 1.0000e-03  eta: 0:38:43  time: 0.6038  data_time: 0.0384  memory: 12672  loss: 1.1237  loss_cls: 0.1556  loss_bbox: 0.4479  loss_dfl: 0.1489  backbone_loss_mask: 0.3079  clip_transfer_loss: 0.0635
2025/07/04 22:08:22 - mmengine - INFO - Epoch(train) [11][300/491]  lr: 1.0000e-03  eta: 0:35:47  time: 0.6026  data_time: 0.0363  memory: 12672  loss: 1.1368  loss_cls: 0.1625  loss_bbox: 0.4424  loss_dfl: 0.1480  backbone_loss_mask: 0.3155  clip_transfer_loss: 0.0685
2025/07/04 22:08:53 - mmengine - INFO - Epoch(train) [11][350/491]  lr: 1.0000e-03  eta: 0:32:54  time: 0.6091  data_time: 0.0345  memory: 12672  loss: 1.1134  loss_cls: 0.1624  loss_bbox: 0.4347  loss_dfl: 0.1504  backbone_loss_mask: 0.2994  clip_transfer_loss: 0.0666
2025/07/04 22:09:23 - mmengine - INFO - Epoch(train) [11][400/491]  lr: 1.0000e-03  eta: 0:30:04  time: 0.6024  data_time: 0.0370  memory: 12672  loss: 1.1127  loss_cls: 0.1495  loss_bbox: 0.4420  loss_dfl: 0.1519  backbone_loss_mask: 0.3030  clip_transfer_loss: 0.0663
2025/07/04 22:09:53 - mmengine - INFO - Epoch(train) [11][450/491]  lr: 1.0000e-03  eta: 0:27:17  time: 0.6035  data_time: 0.0387  memory: 12672  loss: 1.1202  loss_cls: 0.1680  loss_bbox: 0.4290  loss_dfl: 0.1517  backbone_loss_mask: 0.3011  clip_transfer_loss: 0.0705
2025/07/04 22:10:18 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 22:10:18 - mmengine - INFO - Saving checkpoint at 11 epochs
2025/07/04 22:10:29 - mmengine - INFO - Epoch(val) [11][ 50/211]    eta: 0:00:31  time: 0.1954  data_time: 0.0385  memory: 12672  
2025/07/04 22:10:39 - mmengine - INFO - Epoch(val) [11][100/211]    eta: 0:00:21  time: 0.1929  data_time: 0.0367  memory: 1782  
2025/07/04 22:10:49 - mmengine - INFO - Epoch(val) [11][150/211]    eta: 0:00:11  time: 0.1899  data_time: 0.0351  memory: 1782  
2025/07/04 22:10:59 - mmengine - INFO - Epoch(val) [11][200/211]    eta: 0:00:02  time: 0.1987  data_time: 0.0415  memory: 1782  
2025/07/04 22:11:01 - mmengine - INFO - Evaluating bbox...
2025/07/04 22:11:04 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.402 | 0.866  | 0.295  | 0.397 | 0.627 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 22:11:04 - mmengine - INFO - bbox_mAP_copypaste: 0.402 0.866 0.295 0.397 0.627 -1.000
2025/07/04 22:11:04 - mmengine - INFO - Epoch(val) [11][211/211]    coco/UAV_precision: 0.4020  coco/bbox_mAP: 0.4020  coco/bbox_mAP_50: 0.8660  coco/bbox_mAP_75: 0.2950  coco/bbox_mAP_s: 0.3970  coco/bbox_mAP_m: 0.6270  coco/bbox_mAP_l: -1.0000  data_time: 0.0376  time: 0.1927
2025/07/04 22:11:34 - mmengine - INFO - Epoch(train) [12][ 50/491]  lr: 1.0000e-04  eta: 0:22:19  time: 0.6011  data_time: 0.0407  memory: 12672  loss: 1.1178  loss_cls: 0.1567  loss_bbox: 0.4312  loss_dfl: 0.1480  backbone_loss_mask: 0.3039  clip_transfer_loss: 0.0780
2025/07/04 22:12:04 - mmengine - INFO - Epoch(train) [12][100/491]  lr: 1.0000e-04  eta: 0:19:38  time: 0.5984  data_time: 0.0398  memory: 12672  loss: 1.1066  loss_cls: 0.1553  loss_bbox: 0.4353  loss_dfl: 0.1511  backbone_loss_mask: 0.3044  clip_transfer_loss: 0.0606
2025/07/04 22:12:34 - mmengine - INFO - Epoch(train) [12][150/491]  lr: 1.0000e-04  eta: 0:17:00  time: 0.6094  data_time: 0.0414  memory: 12672  loss: 1.0898  loss_cls: 0.1497  loss_bbox: 0.4166  loss_dfl: 0.1492  backbone_loss_mask: 0.3012  clip_transfer_loss: 0.0732
2025/07/04 22:13:04 - mmengine - INFO - Epoch(train) [12][200/491]  lr: 1.0000e-04  eta: 0:14:24  time: 0.6078  data_time: 0.0431  memory: 12672  loss: 1.1226  loss_cls: 0.1635  loss_bbox: 0.4320  loss_dfl: 0.1483  backbone_loss_mask: 0.3106  clip_transfer_loss: 0.0682
2025/07/04 22:13:34 - mmengine - INFO - Epoch(train) [12][250/491]  lr: 1.0000e-04  eta: 0:11:51  time: 0.5919  data_time: 0.0378  memory: 12672  loss: 1.0848  loss_cls: 0.1426  loss_bbox: 0.4133  loss_dfl: 0.1505  backbone_loss_mask: 0.3065  clip_transfer_loss: 0.0718
2025/07/04 22:14:04 - mmengine - INFO - Epoch(train) [12][300/491]  lr: 1.0000e-04  eta: 0:09:19  time: 0.6021  data_time: 0.0389  memory: 12672  loss: 1.0862  loss_cls: 0.1509  loss_bbox: 0.4148  loss_dfl: 0.1482  backbone_loss_mask: 0.3024  clip_transfer_loss: 0.0700
2025/07/04 22:14:34 - mmengine - INFO - Epoch(train) [12][350/491]  lr: 1.0000e-04  eta: 0:06:50  time: 0.5977  data_time: 0.0373  memory: 12672  loss: 1.0931  loss_cls: 0.1516  loss_bbox: 0.4154  loss_dfl: 0.1492  backbone_loss_mask: 0.3058  clip_transfer_loss: 0.0712
2025/07/04 22:15:04 - mmengine - INFO - Epoch(train) [12][400/491]  lr: 1.0000e-04  eta: 0:04:22  time: 0.6029  data_time: 0.0369  memory: 12672  loss: 1.1077  loss_cls: 0.1497  loss_bbox: 0.4387  loss_dfl: 0.1505  backbone_loss_mask: 0.2975  clip_transfer_loss: 0.0712
2025/07/04 22:15:34 - mmengine - INFO - Epoch(train) [12][450/491]  lr: 1.0000e-04  eta: 0:01:57  time: 0.6035  data_time: 0.0367  memory: 12672  loss: 1.0916  loss_cls: 0.1565  loss_bbox: 0.4159  loss_dfl: 0.1490  backbone_loss_mask: 0.3032  clip_transfer_loss: 0.0671
2025/07/04 22:15:59 - mmengine - INFO - Exp name: gfl_r50_fpn_1x_m3fd_20250704_171849
2025/07/04 22:15:59 - mmengine - INFO - Saving checkpoint at 12 epochs
2025/07/04 22:16:11 - mmengine - INFO - Epoch(val) [12][ 50/211]    eta: 0:00:32  time: 0.2039  data_time: 0.0445  memory: 12672  
2025/07/04 22:16:20 - mmengine - INFO - Epoch(val) [12][100/211]    eta: 0:00:22  time: 0.1928  data_time: 0.0367  memory: 1782  
2025/07/04 22:16:30 - mmengine - INFO - Epoch(val) [12][150/211]    eta: 0:00:11  time: 0.1930  data_time: 0.0378  memory: 1782  
2025/07/04 22:16:40 - mmengine - INFO - Epoch(val) [12][200/211]    eta: 0:00:02  time: 0.1927  data_time: 0.0376  memory: 1782  
2025/07/04 22:16:42 - mmengine - INFO - Evaluating bbox...
2025/07/04 22:16:44 - mmengine - INFO - 
+----------+-------+--------+--------+-------+-------+-------+
| category | mAP   | mAP_50 | mAP_75 | mAP_s | mAP_m | mAP_l |
+----------+-------+--------+--------+-------+-------+-------+
| UAV      | 0.409 | 0.871  | 0.311  | 0.404 | 0.633 | nan   |
+----------+-------+--------+--------+-------+-------+-------+
2025/07/04 22:16:44 - mmengine - INFO - bbox_mAP_copypaste: 0.409 0.871 0.311 0.404 0.633 -1.000
2025/07/04 22:16:44 - mmengine - INFO - Epoch(val) [12][211/211]    coco/UAV_precision: 0.4090  coco/bbox_mAP: 0.4090  coco/bbox_mAP_50: 0.8710  coco/bbox_mAP_75: 0.3110  coco/bbox_mAP_s: 0.4040  coco/bbox_mAP_m: 0.6330  coco/bbox_mAP_l: -1.0000  data_time: 0.0383  time: 0.1934
